{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Online-DL-Hedge Backpropagation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPSAKBN3Kwde"
      },
      "source": [
        "import collections\r\n",
        "import json\r\n",
        "import random\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.nn.parameter import Parameter\r\n",
        "\r\n",
        "# from mab import algs"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckOK_NkmclLh"
      },
      "source": [
        "#几个小问题：\r\n",
        "1. 怎么实现online：partial fit\r\n",
        "2. pytorch的基本组成和实现原理: 见wiki"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TaMsOVlrVuj6"
      },
      "source": [
        "class ODL(nn.Module):\r\n",
        "  def __init__(self,features_size,max_num_hidden_layers,qtd_neuron_per_hidden_layer,n_classes,batch_size=1,\r\n",
        "          b=0.99,n=0.01,s=0.2,use_cuda=False):\r\n",
        "    super(ODL,self).__init__()\r\n",
        "\r\n",
        "    if torch.cuda.is_available() and use_cuda:\r\n",
        "      print(\"Using Cuda...\")\r\n",
        "  \r\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() and use_cuda else \"cpu\")\r\n",
        "\r\n",
        "    self.features_size = features_size #input X size\r\n",
        "    self.max_num_hidden_layers = max_num_hidden_layers #L\r\n",
        "    self.qtd_neuron_per_hidden_layer = qtd_neuron_per_hidden_layer #hidden layer output size\r\n",
        "    self.n_classes = n_classes #output Y size\r\n",
        "    self.batch_size = batch_size\r\n",
        "    self.b = Parameter(torch.tensor(b),requires_grad=False).to(self.device) #beta weight decay\r\n",
        "    self.n = Parameter(torch.tensor(n),requires_grad=False).to(self.device) #learning rate\r\n",
        "    self.s = Parameter(torch.tensor(s),requires_grad=False).to(self.device) #smooting parameter\r\n",
        "\r\n",
        "    self.hidden_layers = [] #h(l)\r\n",
        "    self.output_layers = [] #f(l)\r\n",
        "\r\n",
        "    self.hidden_layers.append(nn.Linear(features_size,qtd_neuron_per_hidden_layer)) #h(1)=W(1)*h(0)=W(1)*x\r\n",
        "\r\n",
        "    for i in range(max_num_hidden_layers-1):\r\n",
        "      self.hidden_layers.append(nn.Linear(qtd_neuron_per_hidden_layer,qtd_neuron_per_hidden_layer)) #h(l)=W(l)*h(l-1)\r\n",
        "    \r\n",
        "    for i in range(max_num_hidden_layers):\r\n",
        "      self.output_layers.append(nn.Linear(qtd_neuron_per_hidden_layer,n_classes)) #f(l)=theta(l)*h(l)\r\n",
        "\r\n",
        "    self.hidden_layers = nn.ModuleList(self.hidden_layers).to(self.device) #layer can be indexed like normal python-array\r\n",
        "    self.output_layers = nn.ModuleList(self.output_layers).to(self.device)\r\n",
        "\r\n",
        "    # initialize alpha with 1/L+1 with size: max_num_hidden_layers\r\n",
        "    # torch.Tensor = torch.FloatTensor\r\n",
        "    self.alpha = Parameter(torch.Tensor(self.max_num_hidden_layers).fill_(1/(self.max_num_hidden_layers+1)),requires_grad=False).to(self.device)\r\n",
        "\r\n",
        "    # loss for each classifier f(l)\r\n",
        "    self.loss_array = []\r\n",
        "\r\n",
        "  # all weight and bias in Linear Modules are cleared\r\n",
        "  def zero_grad(self):\r\n",
        "    for i in range(self.max_num_hidden_layers): \r\n",
        "      self.output_layers[i].weight.grad.data.fill_(0)\r\n",
        "      self.output_layers[i].bias.grad.data.fill_(0)\r\n",
        "      self.hidden_layers[i].weight.grad.data.fill_(0)\r\n",
        "      self.hidden_layers[i].bias.grad.data.fill_(0)\r\n",
        "\r\n",
        "  # update weight\r\n",
        "  def update_weights(self,X,Y,show_loss):\r\n",
        "    Y = torch.from_numpy(Y).to(self.device)\r\n",
        "\r\n",
        "    predictions_per_layer = self.forward(X) #f(l) l>=0\r\n",
        "\r\n",
        "    losses_per_layer = []\r\n",
        "\r\n",
        "    for out in predictions_per_layer:\r\n",
        "      criterion = nn.CrossEntropyLoss().to(self.device)\r\n",
        "      loss = criterion(out.view(self.batch_size,self.n_classes),Y.view(self.batch_size).long()) #view: efficient reshape into other dim\r\n",
        "      losses_per_layer.append(loss)\r\n",
        "\r\n",
        "    w = [None]*len(losses_per_layer)\r\n",
        "    b = [None]*len(losses_per_layer)\r\n",
        "\r\n",
        "    # Context-manager that disabled gradient calculation.\r\n",
        "    # Instead we calculate manuelly (should not have Tensor.backward()) --> see: https://stackoverflow.com/questions/60984003/why-the-backpropagation-process-can-still-work-when-i-included-loss-backward\r\n",
        "    # but losses_per_layer[i].backward works, because its defined out of \"no_grad\" and when its defined, its requires_grad=True\r\n",
        "    with torch.no_grad():\r\n",
        "      for i in range(len(losses_per_layer)):\r\n",
        "        # backpropagation to calculate weights and bias once\r\n",
        "        # API highly suggest retain_graph=False,because it's not efficient ---> if we dont need autograd graph, but indeed we do\r\n",
        "        # But in the case of calling backward more than once, we cant delete the graph after finishing once\r\n",
        "        # we need the information from last backpropagation (since gradients should be summed)\r\n",
        "        losses_per_layer[i].backward(retain_graph=True) \r\n",
        "        # update thetha(l) l>=0\r\n",
        "        self.output_layers[i].weight.data -= self.n*self.alpha[i]*self.output_layers[i].weight.grad.data\r\n",
        "        self.output_layers[i].bias.data -= self.n*self.alpha[i]*self.output_layers[i].bias.grad.data\r\n",
        "\r\n",
        "\r\n",
        "        # calculate the summation for W(l) update\r\n",
        "        for j in range(i+1):\r\n",
        "          if w[j] is None:\r\n",
        "            w[j] = self.alpha[i] * self.hidden_layers[j].weight.grad.data\r\n",
        "            b[j] = self.alpha[i] * self.hidden_layers[j].bias.grad.data\r\n",
        "          else:\r\n",
        "            w[j] += self.alpha[i] * self.hidden_layers[j].weight.grad.data\r\n",
        "            b[j] += self.alpha[i] * self.hidden_layers[j].bias.grad.data\r\n",
        "\r\n",
        "        self.zero_grad()\r\n",
        "\r\n",
        "      # update W(l) l>=0\r\n",
        "      for i in range(len(losses_per_layer)):\r\n",
        "        self.hidden_layers[i].weight.data -= self.n*w[i]\r\n",
        "        self.hidden_layers[i].bias.data -= self.n*b[i]\r\n",
        "\r\n",
        "      #update alpha and smooth it\r\n",
        "      for i in range(len(losses_per_layer)):\r\n",
        "        self.alpha[i] *= torch.pow(self.b, losses_per_layer[i]) # update\r\n",
        "        self.alpha[i] = torch.max(self.alpha[i], self.s / self.max_num_hidden_layers) # smooth\r\n",
        "\r\n",
        "      # normalize alpha\r\n",
        "      z_t = torch.sum(self.alpha)\r\n",
        "      self.alpha = Parameter(self.alpha / z_t, requires_grad=False).to(self.device)\r\n",
        "\r\n",
        "    if show_loss:\r\n",
        "      real_output = torch.sum(torch.mul(self.alpha.view(self.max_num_hidden_layers, 1).repeat(1, self.batch_size).view(\r\n",
        "                    self.max_num_hidden_layers, self.batch_size, 1), predictions_per_layer), 0)\r\n",
        "      criterion = nn.CrossEntropyLoss().to(self.device)\r\n",
        "      loss = criterion(real_output.view(self.batch_size, self.n_classes), Y.view(self.batch_size).long())\r\n",
        "      self.loss_array.append(loss)\r\n",
        "      if (len(self.loss_array) % 1000) == 0:\r\n",
        "        print(\"WARNING: Set 'show_loss' to 'False' when not debugging. \"\r\n",
        "                      \"It will deteriorate the fitting performance.\")\r\n",
        "        loss = torch.Tensor(self.loss_array).mean().cpu().numpy()\r\n",
        "        print(\"Alpha:\" + str(self.alpha.data.cpu().numpy()))\r\n",
        "        print(\"Training Loss: \" + str(loss))\r\n",
        "        self.loss_array.clear()\r\n",
        "\r\n",
        "  # forward propagation\r\n",
        "  def forward(self, X):\r\n",
        "    hidden_connections = []\r\n",
        "\r\n",
        "    X = torch.from_numpy(X).float().to(self.device)\r\n",
        "\r\n",
        "    # update h(0)\r\n",
        "    x = F.relu(self.hidden_layers[0](X))\r\n",
        "    hidden_connections.append(x)\r\n",
        "\r\n",
        "    # update all h(l) l>=1\r\n",
        "    for i in range(1, self.max_num_hidden_layers):\r\n",
        "      hidden_connections.append(\r\n",
        "        F.relu(self.hidden_layers[i](hidden_connections[i - 1])))\r\n",
        "\r\n",
        "    output_class = []\r\n",
        "\r\n",
        "    # update all f(l) l>=0\r\n",
        "    for i in range(self.max_num_hidden_layers):\r\n",
        "      output_class.append(self.output_layers[i](hidden_connections[i]))\r\n",
        "\r\n",
        "    pred_per_layer = torch.stack(output_class) #stack output with axis=0\r\n",
        "\r\n",
        "    return pred_per_layer\r\n",
        "\r\n",
        "  def validate_input_X(self, data):\r\n",
        "    if len(data.shape) != 2:\r\n",
        "      raise Exception(\r\n",
        "        \"Wrong dimension for this X data. It should have only two dimensions.\")\r\n",
        "\r\n",
        "  def validate_input_Y(self, data):\r\n",
        "    if len(data.shape) != 1:\r\n",
        "      raise Exception(\r\n",
        "          \"Wrong dimension for this Y data. It should have only one dimensions.\")\r\n",
        "\r\n",
        "  def partial_fit_(self, X_data, Y_data, show_loss=True):\r\n",
        "    self.validate_input_X(X_data)\r\n",
        "    self.validate_input_Y(Y_data)\r\n",
        "    self.update_weights(X_data, Y_data, show_loss)\r\n",
        "\r\n",
        "  def partial_fit(self, X_data, Y_data, show_loss=True):\r\n",
        "    self.partial_fit_(X_data, Y_data, show_loss)\r\n",
        "\r\n",
        "  def predict_(self, X_data):\r\n",
        "    self.validate_input_X(X_data)\r\n",
        "    return torch.argmax(torch.sum(torch.mul(self.alpha.view(self.max_num_hidden_layers, 1).repeat(1, len(X_data)).view( #??\r\n",
        "         self.max_num_hidden_layers, len(X_data), 1), self.forward(X_data)), 0), dim=1).cpu().numpy()\r\n",
        "\r\n",
        "  def predict(self, X_data):\r\n",
        "    pred = self.predict_(X_data)\r\n",
        "    return pred\r\n",
        "\r\n",
        "  def export_params_to_json(self):\r\n",
        "    state_dict = self.state_dict() #torch自带获取参数方法\r\n",
        "    params_gp = {}\r\n",
        "    for key, tensor in state_dict.items():\r\n",
        "      params_gp[key] = tensor.cpu().numpy().tolist()\r\n",
        "\r\n",
        "    return json.dumps(params_gp)\r\n",
        "\r\n",
        "  def load_params_from_json(self, json_data):\r\n",
        "    params = json.loads(json_data)\r\n",
        "    o_dict = collections.OrderedDict() #记住写入顺序的Dict\r\n",
        "    for key, tensor in params.items():\r\n",
        "      o_dict[key] = torch.tensor(tensor).to(self.device)\r\n",
        "    self.load_state_dict(o_dict) #torch方法"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "peSh4QDIbCAt"
      },
      "source": [
        "# Test on fake data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5JKbxFwhVEU"
      },
      "source": [
        "from sklearn.datasets import make_classification, make_circles\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.metrics import accuracy_score,balanced_accuracy_score"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1Ztv0UbhlNA"
      },
      "source": [
        "# initializing network\r\n",
        "odl_network = ODL(features_size=10,max_num_hidden_layers=5,qtd_neuron_per_hidden_layer=40,n_classes=10)\r\n",
        "\r\n",
        "# fake classification Dataset\r\n",
        "X,Y = make_classification(n_samples=50000,n_features=10,n_informative=4,n_redundant=0,n_classes=10,n_clusters_per_class=1,class_sep=3)\r\n",
        "\r\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,Y,test_size=0.3,random_state=42,shuffle=True)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_qG29bVieJf"
      },
      "source": [
        "# learning and predicing\r\n",
        "for i in range(len(X_train)): #由于batch=1，输出有50000*0.7/1000/1=35个\r\n",
        "  odl_network.partial_fit(np.asarray([X_train[i,:]]),np.asarray([y_train[i]]))\r\n",
        "\r\n",
        "  if i%1000 == 0:\r\n",
        "    predictions = odl_network.predict(X_test)\r\n",
        "    print(\"Online Accuracy: {}\".format(balanced_accuracy_score(y_test,predictions)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzvOgNBDkMHw"
      },
      "source": [
        "# learning with CUDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNidJGBfunfK"
      },
      "source": [
        "odl_network = ODL(features_size=10, max_num_hidden_layers=5, qtd_neuron_per_hidden_layer=40, n_classes=10, batch_size=10, use_cuda=True)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKoVQ-_Hu9YT"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "class Dataset(Dataset):\r\n",
        "  def __init__(self,X,y):\r\n",
        "    self.X = X\r\n",
        "    self.y = y\r\n",
        "\r\n",
        "  def __len__(self):\r\n",
        "    return len(self.X)\r\n",
        "\r\n",
        "  def __getitem__(self,idx):\r\n",
        "    X = self.X[idx]\r\n",
        "    Y = self.y[idx]\r\n",
        "\r\n",
        "    return X,Y"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdVUTx3Tvf6Z"
      },
      "source": [
        "transformed_dataset = Dataset(X_train,y_train)\r\n",
        "dataloader = DataLoader(transformed_dataset,batch_size=10,shuffle=True,num_workers=1)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1pyGRvKv3Vv",
        "outputId": "99eabd12-08bb-4c0d-cb55-b4468ac5be72"
      },
      "source": [
        "for local_X,local_Y in dataloader:\r\n",
        "  odl_network.partial_fit(local_X.numpy(),local_Y.numpy()) #由于batch变成10了，输出只有50000*0.7/1000/10=3.5个"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Set 'show_loss' to 'False' when not debugging. It will deteriorate the fitting performance.\n",
            "Alpha:[0.8397107 0.0400723 0.0400723 0.0400723 0.0400723]\n",
            "Training Loss: 1.0670693\n",
            "WARNING: Set 'show_loss' to 'False' when not debugging. It will deteriorate the fitting performance.\n",
            "Alpha:[0.8396637  0.04008406 0.04008406 0.04008406 0.04008406]\n",
            "Training Loss: 0.24557245\n",
            "WARNING: Set 'show_loss' to 'False' when not debugging. It will deteriorate the fitting performance.\n",
            "Alpha:[0.83952117 0.04011968 0.04011968 0.04011968 0.04011968]\n",
            "Training Loss: 0.18645324\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N5WS_GFrwIQj",
        "outputId": "3999ae59-479a-402a-8eb3-608b0b9a565e"
      },
      "source": [
        "predictions = odl_network.predict(X_test)\r\n",
        "print(\"Accuracy: {}\".format(balanced_accuracy_score(y_test, predictions)))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9710902998285205\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmCRgteZwfAG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}