{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Transfer-Learning-Keras-workflow.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP58nadFa44+a6MGsYNb+SS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niemand-01/ML-Demo/blob/master/Transfer_Learning_Keras_workflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MW7M8-L_jf7U"
      },
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kwiR6LXiT8a"
      },
      "source": [
        "# Typical Transfer-learning workflow\n",
        "## standard\n",
        "1. Instantiate a base model and load pre-trained weights into it.\n",
        "2. Freeze all layers in the base model by setting trainable = False.\n",
        "3. Create a new model on top of the output of one (or several) layers from the base model.\n",
        "4. Train your new model on your new dataset.\n",
        "\n",
        "## lightweight\n",
        "1. Instantiate a base model and load pre-trained weights into it.\n",
        "2. Run your new dataset through it and record the output of one (or several) layers from the base model. This is called __feature extraction__.\n",
        "3. Use that output as input data for a new, smaller model.\n",
        "\n",
        "## Pro for lightweight:\n",
        "faster&cheaper\n",
        "## Contra for lightweight:\n",
        "not able to modify the input data of new model dynamically (important for data augmentation)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zSWkr60jZT6"
      },
      "source": [
        "# standard workflow\n",
        "\n",
        "# instantiate a basemodel and load pretrained weights\n",
        "base_model = keras.applications.Xception(\n",
        "    weights = \"imagenet\", # load pretrained weights on ImageNet\n",
        "    input_shape=(150,150,3),\n",
        "    include_top=False\n",
        ")\n",
        "\n",
        "# freeze all weights in base-model\n",
        "base_model.trainable = False\n",
        "\n",
        "# create a new small model on top\n",
        "inputs = keras.Input(shape=(150,150,3))\n",
        "\n",
        "x = base_model(inputs,training=False)\n",
        "\n",
        "x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "outputs = keras.layers.Dense(1)(x)\n",
        "\n",
        "model = keras.Model(inputs,outputs)\n",
        "\n",
        "\n",
        "# train new model\n",
        "model.complie(\n",
        "    optimizer = keras.optimizers.Adam(),\n",
        "    loss = keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "    metrics = [keras.metrics.BinaryAccuracy()]\n",
        ")\n",
        "\n",
        "# model.fit(new_dataset,epochs=20,callbacks=...,validation_data=...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJGyktACnQTD"
      },
      "source": [
        "## Transfer learning & fine-tuning with a custom training loop\n",
        "If instead of fit(), you are using your own low-level training loop, the workflow stays essentially the same. You should be careful to only take into account the list model.trainable_weights when applying gradient updates:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efaHnXITnWn-"
      },
      "source": [
        "# Create base model\n",
        "base_model = keras.applications.Xception(\n",
        "    weights='imagenet',\n",
        "    input_shape=(150, 150, 3),\n",
        "    include_top=False)\n",
        "# Freeze base model\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create new model on top.\n",
        "inputs = keras.Input(shape=(150, 150, 3))\n",
        "x = base_model(inputs, training=False)\n",
        "x = keras.layers.GlobalAveragePooling2D()(x)\n",
        "outputs = keras.layers.Dense(1)(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "optimizer = keras.optimizers.Adam()\n",
        "\n",
        "# Iterate over the batches of a dataset.\n",
        "for inputs, targets in new_dataset:\n",
        "    # Open a GradientTape.\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Forward pass.\n",
        "        predictions = model(inputs)\n",
        "        # Compute the loss value for this batch.\n",
        "        loss_value = loss_fn(targets, predictions)\n",
        "\n",
        "    # Get gradients of loss wrt the *trainable* weights.\n",
        "    gradients = tape.gradient(loss_value, model.trainable_weights)\n",
        "    # Update the weights of the model.\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}