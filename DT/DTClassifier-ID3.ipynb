{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DTClassifier-ID3.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOh4NqF7H8+NEIK+77yzIFE"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"dGWiMyG8xzZH","executionInfo":{"status":"ok","timestamp":1615053972599,"user_tz":-60,"elapsed":476,"user":{"displayName":"k ww","photoUrl":"","userId":"09134966690380548520"}}},"source":["import numpy as np\r\n","import pandas as pd\r\n"],"execution_count":124,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"Ok2xvrjGx722","executionInfo":{"status":"ok","timestamp":1615053973167,"user_tz":-60,"elapsed":1032,"user":{"displayName":"k ww","photoUrl":"","userId":"09134966690380548520"}},"outputId":"9fa5493b-7ae1-4c90-db99-d6a46682dc5f"},"source":["\"\"\"\r\n","  Extend the functionality of a Decision Tree Classifier with numpy integration:\r\n","  Basic Idea comes from: https://github.com/random-forests/tutorials/blob/master/decision_tree.ipynb\r\n","  Numpy and Pandas Extrension from: https://github.com/Nir-J/Decision_tree_ID3/blob/master/dtree.ipynb\r\n","  Added Depth, min_sample_leafs and min_sample_Split from: https://medium.datadriveninvestor.com/easy-implementation-of-decision-tree-with-python-numpy-9ec64f05f8ae\r\n","  \r\n","  Decision Tree Regression Idea from: https://austindavidbrown.github.io/post/2019/01/regression-decision-trees-in-python/\r\n","\"\"\""],"execution_count":125,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n  Extend the functionality of a Decision Tree Classifier with numpy integration:\\n  Basic Idea comes from: https://github.com/random-forests/tutorials/blob/master/decision_tree.ipynb\\n  Numpy and Pandas Extrension from: https://github.com/Nir-J/Decision_tree_ID3/blob/master/dtree.ipynb\\n  Added Depth, min_sample_leafs and min_sample_Split from: https://medium.datadriveninvestor.com/easy-implementation-of-decision-tree-with-python-numpy-9ec64f05f8ae\\n  \\n  Decision Tree Regression Idea from: https://austindavidbrown.github.io/post/2019/01/regression-decision-trees-in-python/\\n'"]},"metadata":{"tags":[]},"execution_count":125}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":34},"id":"ijnEfL4n0CPp","executionInfo":{"status":"ok","timestamp":1615053973168,"user_tz":-60,"elapsed":1013,"user":{"displayName":"k ww","photoUrl":"","userId":"09134966690380548520"}},"outputId":"1039d31a-ef8a-47d4-9d9d-92f01ed3df94"},"source":["\"\"\"\r\n","  Decision Tree:\r\n","  @Input: \r\n","    - DataFrame with size: (rows,cols)\r\n","    - the last column is the target label \r\n","    - Label is either numeric (class number) or string type (\"Apple\")\r\n","    - features are of either numeric or string type\r\n","\"\"\""],"execution_count":126,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n  Decision Tree:\\n  @Input: \\n    - DataFrame with size: (rows,cols)\\n    - the last column is the target label \\n    - Label is either numeric (class number) or string type (\"Apple\")\\n    - features are of either numeric or string type\\n'"]},"metadata":{"tags":[]},"execution_count":126}]},{"cell_type":"code","metadata":{"id":"ha0hkkWiywAL","executionInfo":{"status":"ok","timestamp":1615053973170,"user_tz":-60,"elapsed":1005,"user":{"displayName":"k ww","photoUrl":"","userId":"09134966690380548520"}}},"source":["class Utils:\r\n","  def __init__(self):\r\n","    pass\r\n","\r\n","  def unique_vals(df,col):\r\n","    \"\"\" find unique values for a column in a dataset \"\"\"\r\n","    return np.unique(df.loc[:,col])\r\n","  def class_counts(df):\r\n","    \"\"\" counts the number of each class of target label in a dataset \"\"\"\r\n","    return np.unique(df.iloc[:,-1],return_counts=True)\r\n","\r\n","  def is_numeric(val):\r\n","    \"\"\" test if a value is numeric \"\"\"\r\n","    return isinstance(val,np.int64) or isinstance(val,np.float) or isinstance(val,np.int) or isinstance(val,int)"],"execution_count":127,"outputs":[]},{"cell_type":"code","metadata":{"id":"JBpuaP000AuN","executionInfo":{"status":"ok","timestamp":1615053973170,"user_tz":-60,"elapsed":998,"user":{"displayName":"k ww","photoUrl":"","userId":"09134966690380548520"}}},"source":["class Question:\r\n","  def __init__(self,col_num,val):\r\n","    self.col_num = col_num\r\n","    self.eval_val = val\r\n","  \r\n","  def match(self,df_row):\r\n","    \"\"\"\r\n","      @input: 1-d DataFrame row\r\n","    \"\"\"\r\n","    test_val = df_row[self.col_num] \r\n","    if Utils.is_numeric(test_val):\r\n","      # we choose \">=\" as split criterion for numeric values\r\n","      return test_val >= self.eval_val\r\n","    else:\r\n","      # we choose \"==\" as split criterion for string values\r\n","      return test_val == self.eval_val\r\n","\r\n","  def partition(self,rows):\r\n","    \"\"\"partition the dataset with given question\r\n","    \"\"\"\r\n","    true_rows,false_rows = [],[]\r\n","    for row in rows:\r\n","      if self.match(df_row):\r\n","        true_rows.append(row)\r\n","      else:\r\n","        false_rows.append(row)\r\n","    return true_rows,false_rows\r\n","\r\n","  def df_partition(self,rows):\r\n","    \"\"\"@input: pd.DataFrame\r\n","    @return: pd.DataFrame\r\n","    \"\"\"\r\n","    if Utils.is_numeric(self.eval_val):\r\n","      true_rows,false_rows = rows[rows.iloc[:,self.col_num]>=self.eval_val],rows[rows.iloc[:,self.col_num]<self.eval_val]\r\n","    else:\r\n","      #print(rows[self.col_num]==self.eval_val)\r\n","      true_rows,false_rows = rows[rows.iloc[:,self.col_num]==self.eval_val],rows[~(rows.iloc[:,self.col_num]==self.eval_val)]\r\n","    \r\n","    return true_rows,false_rows\r\n","\r\n","  def classify(self,row):\r\n","    \"\"\"@Input:pandas.Series\"\"\"\r\n","    if Utils.is_numeric(self.eval_val):\r\n","      return row.iloc[self.col_num]>=self.eval_val\r\n","    else:\r\n","      #print(rows[self.col_num]==self.eval_val)\r\n","      return row.iloc[self.col_num]==self.eval_val\r\n","\r\n","\r\n","  def __repr__(self):\r\n","    criterion = \"==\"\r\n","    if Utils.is_numeric(self.eval_val):\r\n","      criterion = \">=\"\r\n","    return f\"Is {df.columns[self.col_num]} {criterion} {self.eval_val} \""],"execution_count":128,"outputs":[]},{"cell_type":"code","metadata":{"id":"amUtRYprBJpK","executionInfo":{"status":"ok","timestamp":1615053973171,"user_tz":-60,"elapsed":995,"user":{"displayName":"k ww","photoUrl":"","userId":"09134966690380548520"}}},"source":["class Gain:\r\n","  def gini(rows):\r\n","    \"\"\"@input: pd.DataFrame\r\n","    \"\"\"\r\n","    names,counts = Utils.class_counts(rows)\r\n","    impurity = 1.0\r\n","    prob_lbl = np.sum((counts/float(len(rows)))**2)\r\n","    impurity -= prob_lbl\r\n","\r\n","    return impurity\r\n","\r\n","  def entropy(rows):\r\n","    names,counts = Utils.class_counts(rows)\r\n","    entropy = 0\r\n","    prob_of_lbl = np.sum(-counts/float(len(rows))*np.log2(counts/float(len(rows))))\r\n","    entropy -= prob_of_lbl\r\n","    return entropy\r\n","\r\n","  def info_gain_gini(left,right,current_uncertainty):\r\n","    \"\"\"\r\n","      @input:\r\n","        left: true rows\r\n","        right: false rows\r\n","        current_uncertainty: gini of current rows\r\n","    \"\"\"\r\n","    p = float(len(left)) / (len(left)+len(right))\r\n","    return current_uncertainty - p*Gain.gini(left) -(1-p)*Gain.gini(right)\r\n","\r\n","  def info_gain_entropy(left,right,current_uncertainty):\r\n","    \"\"\"\r\n","      @input:\r\n","        left: true rows\r\n","        right: false rows\r\n","        current_uncertainty: entropy of current rows\r\n","    \"\"\"\r\n","    p = float(len(left))/(len(left)+len(right)) # percent of left\r\n","    return current_uncertainty - p*Gain.entropy(left) -(1-p)*Gain.entropy(right)"],"execution_count":129,"outputs":[]},{"cell_type":"code","metadata":{"id":"UjBOGZyQI9rN","executionInfo":{"status":"ok","timestamp":1615053973172,"user_tz":-60,"elapsed":992,"user":{"displayName":"k ww","photoUrl":"","userId":"09134966690380548520"}}},"source":["class Leaf:\r\n","    \"\"\"A Leaf node classifies data.\r\n","\r\n","    This holds a dictionary of class (e.g., \"Apple\") -> number of times\r\n","    it appears in the rows from the training data that reach this leaf.\r\n","    \"\"\"\r\n","\r\n","    def __init__(self, rows): \r\n","      counts = np.column_stack(Utils.class_counts(rows)) # == list(zip(v1,v2))\r\n","      self.predictions = {row[0]:row[1] for row in counts} # create dict\r\n","class Node:\r\n","    \"\"\"A Decision Node tests a condition.\r\n","\r\n","    This holds a reference to the condition, and to the two child nodes.\r\n","    \"\"\"\r\n","\r\n","    def __init__(self,condition,\r\n","              true_branch,\r\n","              false_branch):\r\n","        self.condition = condition\r\n","        self.true_branch = true_branch\r\n","        self.false_branch = false_branch"],"execution_count":130,"outputs":[]},{"cell_type":"code","metadata":{"id":"rUH6Hlj7HhE9","executionInfo":{"status":"ok","timestamp":1615053973353,"user_tz":-60,"elapsed":1169,"user":{"displayName":"k ww","photoUrl":"","userId":"09134966690380548520"}}},"source":["class DecisionTreeClassifierMy:\r\n","  def __init__(self,max_depth=32,min_sample_split=2):\r\n","    self.max_depth = max_depth\r\n","    self.min_sample_split = min_sample_split\r\n","\r\n","  def train(self,rows):\r\n","    \"\"\" build a tree with given data\"\"\"\r\n","    self.root = self._build_tree(rows,0)\r\n","    return self.root\r\n","\r\n","  def find_best_split(self,rows):\r\n","    \"\"\"\r\n","    Find the best question to ask by iterating over every feature / value\r\n","    and calculating the information gain.\r\n","    \r\n","    @input:\r\n","      rows: pd.DataFrame\r\n","    \"\"\"\r\n","    best_gain = 0  # keep track of the best information gain\r\n","    best_question = None  # keep train of the feature / value that produced it\r\n","    current_uncertainty = Gain.gini(rows)\r\n","    n_features = len(rows.columns)-1  # number of features\r\n","\r\n","    for col in range(n_features):  # for each feature\r\n","\r\n","        values = np.unique(rows.iloc[:,col]) # unique values in the column\r\n","        for val in values:  \r\n","            question = Question(col, val)\r\n","            # try splitting the dataset\r\n","            true_rows, false_rows = question.df_partition(rows)\r\n","\r\n","            # Skip this split if it doesn't divide the dataset.\r\n","            if len(true_rows) == 0 or len(false_rows) == 0:\r\n","                continue\r\n","\r\n","            # Calculate the information gain from this split\r\n","            gain = Gain.info_gain_gini(true_rows, false_rows, current_uncertainty)\r\n","\r\n","            if gain >= best_gain:\r\n","                best_gain, best_question = gain, question\r\n","\r\n","    return best_gain, best_question\r\n","\r\n","  def _build_tree(self,rows,depth):\r\n","    \"\"\"\r\n","      Recurvisely split trees and test condition\r\n","\r\n","      @input: pd.DataFrame\r\n","    \"\"\"\r\n","    # if number of samples is less than required, we cannot split\r\n","    if df.shape[0] < self.min_sample_split:\r\n","      return Leaf(rows)\r\n","\r\n","    gain, question = self.find_best_split(rows)\r\n","\r\n","    # at the recursion end: no gain obtained\r\n","    # or depth violated\r\n","    # --> return leaf\r\n","    if gain == 0 or depth == self.max_depth:\r\n","        return Leaf(rows)\r\n","\r\n","    # partition recursively with best condition and pass \"depth\" control\r\n","    true_rows, false_rows = question.df_partition(rows)\r\n","    true_branch = self._build_tree(true_rows,depth+1) # node true\r\n","    false_branch = self._build_tree(false_rows,depth+1) # node false\r\n","\r\n","    print(\"splitting currend node depth %d\"%(depth))\r\n","\r\n","    return Node(question, true_branch, false_branch)\r\n","\r\n","\r\n","\r\n","  def predict_one(self,row):\r\n","    \"\"\" return prediction for single row \"\"\"\r\n","    prediction = self._classify(row,self.root)\r\n","    return prediction[\"label\"]\r\n","\r\n","  def predict(self,rows):\r\n","    predictions = []\r\n","    for idx in rows.index:\r\n","      print(idx)\r\n","      predictions.append(self.predict_one(rows.loc[idx,:]))\r\n","    return predictions\r\n","\r\n","\r\n","  def _classify(self,row,node):\r\n","    # base case: at a leaf node\r\n","    if isinstance(node,Leaf):\r\n","      maxcount = 0\r\n","      maxlabel = None\r\n","      sum = 0\r\n","      \r\n","      for k,v in node.predictions.items():\r\n","        sum+=v\r\n","        \r\n","        if v>=maxcount:\r\n","          maxcount=v\r\n","          maxlabel=k\r\n","      return {\"label\":maxlabel,\"with probability\":float(maxcount/sum)}\r\n","\r\n","    # climb down the tree with condition\r\n","    if isinstance(node,Node):\r\n","      condition = node.condition.classify(row)\r\n","      \r\n","      if condition:\r\n","        return self._classify(row,node.true_branch) # need add return at each branch if we want a return at the end\r\n","      else:\r\n","        return self._classify(row,node.false_branch)\r\n","    \r\n","\r\n","  def print_tree(self,node, spacing=\"\"):\r\n","    \"\"\"World's most elegant tree printing function.\"\"\"\r\n","\r\n","    # Base case: we've reached a leaf\r\n","    if isinstance(node, Leaf):\r\n","        print (spacing + \"Predict\", node.predictions)\r\n","        return\r\n","\r\n","    # Print the question at this node\r\n","    print (spacing + str(node.condition))\r\n","\r\n","    # Call this function recursively on the true branch\r\n","    print (spacing + '--> True:')\r\n","    self.print_tree(node.true_branch, spacing + \"  \")\r\n","\r\n","    # Call this function recursively on the false branch\r\n","    print (spacing + '--> False:')\r\n","    self.print_tree(node.false_branch, spacing + \"  \")"],"execution_count":131,"outputs":[]},{"cell_type":"code","metadata":{"id":"h4tNDQAYqeJ-","executionInfo":{"status":"ok","timestamp":1615053973355,"user_tz":-60,"elapsed":1165,"user":{"displayName":"k ww","photoUrl":"","userId":"09134966690380548520"}}},"source":["class Metrics:\r\n","  def accuracy(preds,labels):\r\n","    \"\"\"Calculate acc given two numpy arrays in percent\"\"\"\r\n","    return np.mean(preds == labels)*100\r\n","\r\n","  def confusion_matrix(preds,labels):\r\n","    \"\"\"Calculate the confusion matrix\r\n","      @properties:\r\n","        Maximal 10 classes\r\n","        if class=2: TP,FP matrix\r\n","    \"\"\"\r\n","    matrix = np.zeros((10,10))\r\n","    for i in range(len(preds)):\r\n","      matrix[preds[i],labels[i]]+=1\r\n","    return matrix\r\n","\r\n","  def precision_and_recall(preds,labels):\r\n","    \"\"\"Returns individual Precion and Recall values of each class\r\n","    \r\n","      @Properties:\r\n","        Precision: TP/TP+FP\r\n","        Recall: TP/TP+FN\r\n","        f1: 2*P*R/P+R\r\n","\r\n","      @Confusion Matrix:class=2\r\n","      TP|FP\r\n","      FN|FP\r\n","        \r\n","    \"\"\"\r\n","    matrix = Metrics.confusion_matrix(preds,labels)\r\n","    r,p =[],[]\r\n","    for i in range(10):\r\n","      TP = float(matrix[i,i])\r\n","      TP_FP = np.sum(matrix[i,:])\r\n","      TP_FN = np.sum(matrix[:,i])\r\n","\r\n","      recall = np.nan if TP_FN == 0 else TP/TP_FN\r\n","      precision = np.nan if TP_FP == 0 else TP/TP_FP\r\n","      r.append(recall)\r\n","      p.append(precision)\r\n","    \r\n","    return (p, r)\r\n","\r\n","  def precision_and_recall_score(preds,labels):\r\n","    \"\"\"Calculate Macro Version of Precision and Recall\"\"\"\r\n","    p,r = Metrics.precision_and_recall(preds,labels)\r\n","    p_score,r_score = 0,0\r\n","    last_class_index = 0\r\n","    for i,val in enumerate(p):\r\n","      if ~np.isnan(val): \r\n","        p_score += p[i]\r\n","        if i>last_class_index:\r\n","          last_class_index = i\r\n","    for i,val in enumerate(r):\r\n","      if ~np.isnan(val):\r\n","        r_score += r[i]\r\n","        if i>last_class_index:\r\n","          last_class_index = i\r\n","    class_count = last_class_index +1  \r\n","        \r\n","    return float(p_score/class_count),float(r_score/class_count)\r\n","\r\n","  def f1score(preds, labels):\r\n","    \"\"\"Calculates macro f1 score given two numpy arrays\"\"\"\r\n","    \r\n","    precision,recall = Metrics.precision_and_recall_score(preds,labels)\r\n","    return 2*precision*recall/(precision+recall)"],"execution_count":132,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WuicJM2K8dfK","executionInfo":{"status":"ok","timestamp":1615053973359,"user_tz":-60,"elapsed":1163,"user":{"displayName":"k ww","photoUrl":"","userId":"09134966690380548520"}},"outputId":"dc66f620-a11f-4907-99cb-18f9b39d5132"},"source":["# test_metrics\r\n","from sklearn.metrics import precision_score,recall_score,f1_score\r\n","a = [0,1,0,1,1,2]\r\n","b = [0,1,1,1,1,3]\r\n","print(Metrics.confusion_matrix(a,b))\r\n","print(Metrics.precision_and_recall(a,b))\r\n","print(Metrics.precision_and_recall_score(a,b))\r\n","print(Metrics.f1score(a,b))\r\n","print(precision_score(a,b,average='macro'))\r\n","print(recall_score(a,b,average='macro'))\r\n","print(f1_score(a,b,average='macro'))\r\n"],"execution_count":133,"outputs":[{"output_type":"stream","text":["[[1. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 3. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n","([0.5, 1.0, 0.0, nan, nan, nan, nan, nan, nan, nan], [1.0, 0.75, nan, 0.0, nan, nan, nan, nan, nan, nan])\n","(0.375, 0.4375)\n","0.40384615384615385\n","0.4375\n","0.375\n","0.38095238095238093\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"OE5N2gWjhOlk"},"source":["# test main"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":190},"id":"nQuLGHPN4uER","executionInfo":{"status":"ok","timestamp":1615053973361,"user_tz":-60,"elapsed":1156,"user":{"displayName":"k ww","photoUrl":"","userId":"09134966690380548520"}},"outputId":"68b5e6f7-69db-4ac1-8ce7-0531c832fa8a"},"source":["# test_main\r\n","\r\n","_data = [\r\n","    ['Green', 3, 'Apple'],\r\n","    ['Yellow', 3, 'Apple'],\r\n","    ['Red', 1, 'Grape'],\r\n","    ['Red', 1, 'Grape'],\r\n","    ['Yellow', 3, 'Lemon'],\r\n","] \r\n","\r\n","df = pd.DataFrame(_data,columns=[\"color\", \"diameter\", \"label\"])\r\n","print(df.shape)\r\n","test_df = pd.DataFrame([['Green', 3, 'Apple']],columns=[\"color\", \"diameter\", \"label\"])\r\n","\r\n","\r\n","print(df.loc[:,\"color\"])\r\n","print([\"red\",\"green\"]!=\"green\")\r\n","\r\n","q1 = Question(1,2)\r\n","\"\"\"\r\n","true,false =q1.df_partition(df)\r\n","print(true)\r\n","print(false)\r\n","\r\n","no_mixing = [['Apple'],['Orange']]\r\n","no = pd.DataFrame(no_mixing)\r\n","gg = Gain()\r\n","print(gg.gini(no))\r\n","\r\n","print(Gain.gini(df))\r\n","\r\n","dt = DecisionTreeClassifier()\r\n","mytree = dt.train(df)\r\n","print(mytree)\r\n","dt.print_tree(mytree)\r\n","\r\n","print(dt.predict(test_df))\r\n","\"\"\""],"execution_count":134,"outputs":[{"output_type":"stream","text":["(5, 3)\n","0     Green\n","1    Yellow\n","2       Red\n","3       Red\n","4    Yellow\n","Name: color, dtype: object\n","True\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\ntrue,false =q1.df_partition(df)\\nprint(true)\\nprint(false)\\n\\nno_mixing = [['Apple'],['Orange']]\\nno = pd.DataFrame(no_mixing)\\ngg = Gain()\\nprint(gg.gini(no))\\n\\nprint(Gain.gini(df))\\n\\ndt = DecisionTreeClassifier()\\nmytree = dt.train(df)\\nprint(mytree)\\ndt.print_tree(mytree)\\n\\nprint(dt.predict(test_df))\\n\""]},"metadata":{"tags":[]},"execution_count":134}]},{"cell_type":"code","metadata":{"id":"gV77UMhN5E-r","executionInfo":{"status":"ok","timestamp":1615053973362,"user_tz":-60,"elapsed":1149,"user":{"displayName":"k ww","photoUrl":"","userId":"09134966690380548520"}}},"source":["# generate random samples\r\n","def generate_random_regression(n_samples=100,n_features=10):\r\n","  \"\"\" implement: y=kx+b+noise \"\"\"\r\n","  intercept = 5*np.ones(n_features)\r\n","  B = 3*np.ones((n_features,1))\r\n","  X = np.zeros((n_samples,n_features))\r\n","  for i in range(0,n_samples):\r\n","    X[i,:] = np.random.multivariate_normal(np.zeros(n_features),10*np.identity(n_features)) #mean=0,cov=identity matrix\r\n","  noise = np.random.multivariate_normal(np.zeros(n_features),np.identity(n_features)) \r\n","  print(X.shape,B.shape)\r\n","  y = intercept + X@B + noise # @ = matmul\r\n","\r\n","  #train test split\r\n","  n_train = int(.7*n_samples)\r\n","  I = np.arange(0,n_samples)\r\n","  train_idx = np.random.choice(I,n_train,replace=False)\r\n","  test_idx = np.setdiff1d(I,train_idx)\r\n","\r\n","  return X[train_idx,:],y[train_idx],X[test_idx,:],y[test_idx]\r\n","\r\n","def generate_random_classifier(n_samples=100,n_features=10):\r\n","  \"\"\" implement: y=kx+b+noise \"\"\"\r\n","  intercept = 5*np.ones(n_features)\r\n","  B = 3*np.ones((n_features,1))\r\n","  X = np.zeros((n_samples,n_features))\r\n","  for i in range(0,n_samples):\r\n","    X[i,:] = np.random.multivariate_normal(np.zeros(n_features),10*np.identity(n_features)) #mean=0,cov=identity matrix\r\n","  y = np.random.randint(low=0,high=2,size=(n_samples,1)) # @ = matmul/dot in 2-D\r\n","\r\n","  #train test split\r\n","  n_train = int(.7*n_samples)\r\n","  I = np.arange(0,n_samples)\r\n","  train_idx = np.random.choice(I,n_train,replace=False)\r\n","  test_idx = np.setdiff1d(I,train_idx)\r\n","\r\n","  return X[train_idx,:],y[train_idx],X[test_idx,:],y[test_idx]\r\n","\r\n","xtrain,ytrain,xtest,ytest = generate_random_classifier()\r\n"],"execution_count":135,"outputs":[]},{"cell_type":"code","metadata":{"id":"enqmuw5_YhKC","executionInfo":{"status":"ok","timestamp":1615053973362,"user_tz":-60,"elapsed":1145,"user":{"displayName":"k ww","photoUrl":"","userId":"09134966690380548520"}}},"source":["df = pd.read_csv(\"winequality-white.csv\",delimiter=\";\")\r\n","df.head()\r\n","\r\n","n_samples = df.shape[0]\r\n","n_train = int(.7*n_samples)\r\n","I = np.arange(0,n_samples)\r\n","train_idx = np.random.choice(I,n_train,replace=False)\r\n","test_idx = np.setdiff1d(I,train_idx)\r\n","\r\n","xtrain,ytrain,xtest,ytest = df.iloc[train_idx,:-1],df.iloc[train_idx,-1],df.iloc[test_idx,:-1],df.iloc[test_idx,-1]"],"execution_count":136,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VcOwUVpn5Flm","executionInfo":{"status":"ok","timestamp":1615054155765,"user_tz":-60,"elapsed":183542,"user":{"displayName":"k ww","photoUrl":"","userId":"09134966690380548520"}},"outputId":"b3528211-7ca3-4437-cb56-f8c48d88eeec"},"source":["dt = DecisionTreeClassifierMy(max_depth=10)\r\n","mytree = dt.train(df.iloc[train_idx])\r\n","# dt.print_tree(mytree)"],"execution_count":137,"outputs":[{"output_type":"stream","text":["splitting currend node depth 6\n","splitting currend node depth 5\n","splitting currend node depth 4\n","splitting currend node depth 3\n","splitting currend node depth 9\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 6\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 6\n","splitting currend node depth 5\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 9\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 6\n","splitting currend node depth 7\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 6\n","splitting currend node depth 5\n","splitting currend node depth 4\n","splitting currend node depth 6\n","splitting currend node depth 7\n","splitting currend node depth 6\n","splitting currend node depth 5\n","splitting currend node depth 9\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 6\n","splitting currend node depth 6\n","splitting currend node depth 5\n","splitting currend node depth 4\n","splitting currend node depth 3\n","splitting currend node depth 2\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 9\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 6\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 6\n","splitting currend node depth 5\n","splitting currend node depth 7\n","splitting currend node depth 6\n","splitting currend node depth 9\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 9\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 9\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 6\n","splitting currend node depth 5\n","splitting currend node depth 4\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 9\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 9\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 6\n","splitting currend node depth 7\n","splitting currend node depth 9\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 9\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 6\n","splitting currend node depth 5\n","splitting currend node depth 7\n","splitting currend node depth 7\n","splitting currend node depth 6\n","splitting currend node depth 5\n","splitting currend node depth 4\n","splitting currend node depth 3\n","splitting currend node depth 8\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 6\n","splitting currend node depth 5\n","splitting currend node depth 6\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 6\n","splitting currend node depth 5\n","splitting currend node depth 4\n","splitting currend node depth 7\n","splitting currend node depth 6\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 6\n","splitting currend node depth 5\n","splitting currend node depth 4\n","splitting currend node depth 3\n","splitting currend node depth 2\n","splitting currend node depth 1\n","splitting currend node depth 8\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 6\n","splitting currend node depth 9\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 7\n","splitting currend node depth 6\n","splitting currend node depth 5\n","splitting currend node depth 7\n","splitting currend node depth 6\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 6\n","splitting currend node depth 5\n","splitting currend node depth 4\n","splitting currend node depth 8\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 8\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 6\n","splitting currend node depth 7\n","splitting currend node depth 6\n","splitting currend node depth 5\n","splitting currend node depth 9\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 7\n","splitting currend node depth 6\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 6\n","splitting currend node depth 5\n","splitting currend node depth 4\n","splitting currend node depth 3\n","splitting currend node depth 6\n","splitting currend node depth 9\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 8\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 6\n","splitting currend node depth 5\n","splitting currend node depth 9\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 9\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 6\n","splitting currend node depth 6\n","splitting currend node depth 5\n","splitting currend node depth 4\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 6\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 6\n","splitting currend node depth 5\n","splitting currend node depth 6\n","splitting currend node depth 6\n","splitting currend node depth 5\n","splitting currend node depth 4\n","splitting currend node depth 3\n","splitting currend node depth 2\n","splitting currend node depth 5\n","splitting currend node depth 8\n","splitting currend node depth 9\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 6\n","splitting currend node depth 8\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 6\n","splitting currend node depth 5\n","splitting currend node depth 4\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 6\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 6\n","splitting currend node depth 5\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 9\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 6\n","splitting currend node depth 9\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 9\n","splitting currend node depth 9\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 6\n","splitting currend node depth 5\n","splitting currend node depth 4\n","splitting currend node depth 3\n","splitting currend node depth 6\n","splitting currend node depth 5\n","splitting currend node depth 4\n","splitting currend node depth 8\n","splitting currend node depth 7\n","splitting currend node depth 6\n","splitting currend node depth 5\n","splitting currend node depth 5\n","splitting currend node depth 4\n","splitting currend node depth 3\n","splitting currend node depth 2\n","splitting currend node depth 1\n","splitting currend node depth 0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"77wbIi-nhb-O"},"source":["# myClassfier"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5tHuwwiLrF5R","executionInfo":{"status":"ok","timestamp":1615054155767,"user_tz":-60,"elapsed":183537,"user":{"displayName":"k ww","photoUrl":"","userId":"09134966690380548520"}},"outputId":"69347918-7a5a-48f0-ef58-4736c4153f96"},"source":["row = xtrain.iloc[0]\r\n","print(row)\r\n","row.iloc[0]"],"execution_count":138,"outputs":[{"output_type":"stream","text":["fixed acidity             6.90000\n","volatile acidity          0.54000\n","citric acid               0.26000\n","residual sugar           12.70000\n","chlorides                 0.04900\n","free sulfur dioxide      59.00000\n","total sulfur dioxide    195.00000\n","density                   0.99596\n","pH                        3.26000\n","sulphates                 0.54000\n","alcohol                  10.50000\n","Name: 4639, dtype: float64\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["6.9"]},"metadata":{"tags":[]},"execution_count":138}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vmPDQp0WvAR6","executionInfo":{"status":"ok","timestamp":1615054245789,"user_tz":-60,"elapsed":443,"user":{"displayName":"k ww","photoUrl":"","userId":"09134966690380548520"}},"outputId":"3c1a4105-4e24-44eb-e52e-8318d040b4bd"},"source":["ytrain"],"execution_count":143,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4639    6\n","1412    8\n","1857    5\n","1829    7\n","783     8\n","       ..\n","943     7\n","3005    6\n","2331    5\n","1041    5\n","3475    6\n","Name: quality, Length: 3428, dtype: int64"]},"metadata":{"tags":[]},"execution_count":143}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Cs2hs16he3o","executionInfo":{"status":"ok","timestamp":1615054157799,"user_tz":-60,"elapsed":185567,"user":{"displayName":"k ww","photoUrl":"","userId":"09134966690380548520"}},"outputId":"9ef77843-03c2-428f-c76f-ff0fae6477a1"},"source":["y_pred = dt.predict(xtrain)"],"execution_count":139,"outputs":[{"output_type":"stream","text":["4639\n","1412\n","1857\n","1829\n","783\n","1361\n","637\n","3738\n","1798\n","3694\n","4062\n","3702\n","4413\n","2436\n","3448\n","203\n","1229\n","223\n","4471\n","155\n","1475\n","1837\n","2748\n","4619\n","1330\n","670\n","361\n","4188\n","3405\n","2530\n","216\n","1830\n","2861\n","4104\n","884\n","3021\n","3176\n","3393\n","346\n","2348\n","4780\n","3034\n","3235\n","1122\n","2864\n","775\n","976\n","1048\n","3945\n","1924\n","434\n","2011\n","3462\n","1682\n","555\n","2485\n","3059\n","4473\n","4863\n","1303\n","2429\n","4446\n","1090\n","4267\n","535\n","610\n","4552\n","2643\n","1079\n","3958\n","4491\n","4865\n","1577\n","4235\n","1120\n","4132\n","2449\n","605\n","3368\n","2795\n","4089\n","24\n","3837\n","890\n","4205\n","1308\n","3861\n","2495\n","3498\n","3992\n","1678\n","4177\n","2788\n","1159\n","158\n","895\n","1716\n","1296\n","4291\n","4754\n","4278\n","4125\n","13\n","4087\n","4401\n","4741\n","4756\n","2303\n","1770\n","1083\n","4897\n","1790\n","4253\n","3447\n","4103\n","4574\n","2020\n","2645\n","2154\n","3273\n","3314\n","2032\n","3487\n","3894\n","1823\n","436\n","947\n","2600\n","2707\n","2300\n","1874\n","2362\n","3800\n","3009\n","1128\n","2629\n","3803\n","4628\n","409\n","3941\n","3269\n","1075\n","3908\n","862\n","1194\n","3337\n","4166\n","3427\n","4882\n","4614\n","4747\n","2672\n","4638\n","131\n","1012\n","649\n","4268\n","2700\n","3111\n","2454\n","488\n","1038\n","4701\n","828\n","3508\n","4057\n","2842\n","1803\n","1334\n","1163\n","2871\n","1454\n","998\n","3715\n","4451\n","2903\n","2474\n","4313\n","2891\n","4644\n","964\n","3177\n","1143\n","2778\n","2666\n","1648\n","4822\n","4435\n","1204\n","2024\n","3580\n","4269\n","1721\n","169\n","3900\n","1752\n","4090\n","817\n","3415\n","3233\n","3012\n","532\n","246\n","464\n","806\n","3647\n","3924\n","2708\n","816\n","928\n","2027\n","3025\n","1400\n","1522\n","864\n","3688\n","2501\n","2183\n","1667\n","4769\n","1408\n","764\n","4028\n","3730\n","1336\n","1736\n","178\n","2476\n","2719\n","1714\n","2942\n","2956\n","1553\n","132\n","3068\n","4150\n","2407\n","1237\n","683\n","825\n","4669\n","4352\n","102\n","4445\n","1992\n","189\n","1483\n","868\n","3668\n","4282\n","1854\n","288\n","3392\n","2569\n","536\n","3524\n","3670\n","3875\n","4767\n","147\n","4838\n","2408\n","2089\n","482\n","1363\n","1328\n","773\n","324\n","2065\n","4645\n","4492\n","2425\n","1331\n","3496\n","3263\n","2625\n","2420\n","3387\n","2770\n","3352\n","3212\n","2714\n","3940\n","4414\n","703\n","1509\n","3315\n","2049\n","4602\n","4441\n","2952\n","1477\n","2206\n","3986\n","1523\n","1681\n","4851\n","2702\n","4518\n","2893\n","1814\n","4580\n","2506\n","843\n","3459\n","2798\n","4426\n","1546\n","4306\n","4807\n","2375\n","272\n","1254\n","4415\n","4112\n","3876\n","2071\n","2651\n","1910\n","3780\n","950\n","209\n","1633\n","2602\n","3180\n","3509\n","681\n","1396\n","2000\n","3295\n","1620\n","2209\n","2044\n","3334\n","3270\n","4697\n","1411\n","1100\n","3406\n","3857\n","3573\n","2541\n","4887\n","2256\n","939\n","1397\n","1186\n","385\n","3467\n","1822\n","3284\n","248\n","4708\n","2695\n","1605\n","1243\n","4059\n","3683\n","2937\n","1873\n","3845\n","338\n","4502\n","3624\n","3530\n","2084\n","3471\n","4566\n","1405\n","2613\n","4266\n","3882\n","585\n","282\n","856\n","1748\n","2979\n","768\n","1133\n","2901\n","3224\n","3156\n","1474\n","3761\n","3477\n","3360\n","3036\n","1244\n","4778\n","2336\n","1493\n","652\n","797\n","2820\n","4477\n","3396\n","3782\n","2556\n","2970\n","526\n","2711\n","874\n","4729\n","1028\n","31\n","999\n","4130\n","452\n","941\n","4734\n","529\n","2926\n","1007\n","2323\n","2591\n","3889\n","4101\n","3937\n","2276\n","4070\n","3121\n","712\n","710\n","3851\n","2247\n","1503\n","29\n","367\n","2494\n","4023\n","4598\n","4862\n","2885\n","769\n","1486\n","1161\n","1259\n","3402\n","1260\n","2913\n","3595\n","2056\n","3014\n","3143\n","1863\n","1715\n","626\n","3104\n","2327\n","656\n","2251\n","143\n","3222\n","1459\n","1471\n","3572\n","3737\n","3959\n","3297\n","2257\n","2687\n","2212\n","3250\n","1231\n","761\n","1750\n","339\n","4403\n","3565\n","3606\n","2760\n","4776\n","2511\n","1738\n","1864\n","631\n","1868\n","2087\n","2922\n","1008\n","298\n","616\n","711\n","1610\n","4086\n","53\n","2289\n","4339\n","2282\n","4198\n","4509\n","612\n","2479\n","251\n","778\n","1722\n","819\n","1433\n","1783\n","3773\n","3559\n","1647\n","3990\n","2325\n","2077\n","2124\n","3541\n","4779\n","3022\n","2198\n","2314\n","1641\n","237\n","1524\n","1962\n","3345\n","4049\n","3846\n","428\n","456\n","455\n","4833\n","3926\n","4419\n","2544\n","3469\n","1494\n","4508\n","620\n","3249\n","1063\n","3514\n","2451\n","386\n","3717\n","4732\n","3461\n","1448\n","4383\n","1071\n","4536\n","1997\n","1086\n","128\n","3810\n","3611\n","1466\n","4885\n","1859\n","1121\n","1278\n","934\n","50\n","3892\n","1498\n","3801\n","5\n","515\n","4534\n","4249\n","1912\n","3934\n","2108\n","513\n","2457\n","3721\n","3150\n","2732\n","4347\n","533\n","4886\n","3651\n","2699\n","1801\n","1673\n","1238\n","1655\n","3929\n","3152\n","2443\n","2284\n","2558\n","2548\n","2968\n","2241\n","1222\n","2815\n","729\n","1192\n","2766\n","3053\n","2551\n","4356\n","4733\n","1879\n","252\n","4540\n","2988\n","558\n","4860\n","3079\n","2104\n","520\n","2582\n","2461\n","3639\n","970\n","878\n","2895\n","225\n","2983\n","1970\n","466\n","1096\n","2603\n","4703\n","918\n","1266\n","3828\n","125\n","4705\n","1088\n","278\n","218\n","600\n","418\n","2261\n","454\n","3722\n","422\n","3122\n","2199\n","3744\n","1972\n","3708\n","3794\n","2238\n","2168\n","4599\n","161\n","2652\n","2744\n","1979\n","1886\n","188\n","1677\n","3560\n","2727\n","3563\n","2100\n","2836\n","3058\n","3439\n","4315\n","705\n","3856\n","2845\n","2283\n","3788\n","696\n","3914\n","2350\n","3966\n","2105\n","4735\n","981\n","4679\n","3685\n","491\n","22\n","159\n","3760\n","3221\n","2870\n","163\n","1884\n","3930\n","3300\n","1988\n","617\n","353\n","861\n","499\n","4805\n","3608\n","742\n","3371\n","1551\n","1210\n","537\n","1558\n","2418\n","3044\n","3672\n","2144\n","2621\n","1731\n","177\n","1359\n","389\n","4555\n","3217\n","4080\n","672\n","528\n","3029\n","1340\n","1356\n","980\n","1753\n","2594\n","4218\n","4539\n","70\n","139\n","1214\n","3322\n","1637\n","839\n","1305\n","2194\n","2742\n","3279\n","219\n","2136\n","1889\n","3323\n","408\n","3712\n","2790\n","2417\n","4226\n","3171\n","4142\n","3055\n","3696\n","4736\n","2021\n","2278\n","3988\n","2962\n","3354\n","3812\n","2931\n","305\n","2857\n","1357\n","2192\n","1378\n","2764\n","145\n","650\n","1398\n","1528\n","4891\n","2170\n","2498\n","4207\n","4694\n","2971\n","1536\n","1772\n","4453\n","4503\n","2785\n","2312\n","2974\n","14\n","4066\n","924\n","226\n","2355\n","621\n","1422\n","2834\n","3226\n","4382\n","4136\n","3219\n","4159\n","700\n","969\n","2804\n","4379\n","1320\n","1936\n","2725\n","2250\n","2337\n","2941\n","1395\n","2244\n","3832\n","4195\n","3024\n","3183\n","787\n","2132\n","1549\n","2907\n","3987\n","3726\n","3681\n","3289\n","1423\n","3764\n","2112\n","4427\n","3054\n","368\n","3299\n","3655\n","4629\n","3115\n","734\n","880\n","556\n","2858\n","1797\n","2119\n","571\n","733\n","4158\n","2572\n","3166\n","3667\n","3747\n","699\n","680\n","1652\n","3272\n","435\n","51\n","4818\n","3536\n","900\n","894\n","2113\n","3428\n","1821\n","3117\n","2609\n","2612\n","4470\n","3549\n","1828\n","3128\n","1824\n","1462\n","65\n","3841\n","2552\n","3291\n","4772\n","1968\n","107\n","2389\n","3106\n","919\n","3041\n","2195\n","333\n","401\n","2975\n","1554\n","3231\n","1288\n","236\n","3706\n","372\n","4500\n","1934\n","3645\n","1723\n","1272\n","805\n","4434\n","1702\n","3497\n","4334\n","172\n","1196\n","2295\n","2432\n","3210\n","3887\n","544\n","3051\n","3657\n","2030\n","64\n","3567\n","1453\n","2098\n","1940\n","573\n","4727\n","991\n","3703\n","3304\n","3817\n","28\n","1930\n","4690\n","3329\n","2142\n","344\n","375\n","1479\n","892\n","2166\n","379\n","933\n","256\n","1672\n","1517\n","1321\n","3347\n","2187\n","1776\n","4354\n","4890\n","561\n","1202\n","523\n","400\n","979\n","664\n","4440\n","2090\n","4189\n","1516\n","4893\n","662\n","299\n","1833\n","1109\n","2402\n","2990\n","1439\n","81\n","3016\n","1178\n","1391\n","3880\n","2562\n","2622\n","4312\n","8\n","3430\n","2528\n","2007\n","3819\n","2840\n","2302\n","3216\n","2723\n","4635\n","2575\n","3802\n","3909\n","1508\n","1174\n","1013\n","105\n","1184\n","2230\n","1112\n","1636\n","2439\n","2865\n","3590\n","3710\n","1511\n","2808\n","2543\n","3533\n","1281\n","26\n","283\n","1058\n","4576\n","1820\n","1317\n","4004\n","1937\n","4480\n","1749\n","1410\n","2371\n","4274\n","2193\n","776\n","1318\n","974\n","2203\n","4060\n","753\n","590\n","398\n","380\n","3256\n","4328\n","4047\n","4875\n","824\n","258\n","1888\n","56\n","2838\n","1148\n","1902\n","4712\n","4714\n","351\n","1640\n","3260\n","1973\n","1921\n","2152\n","870\n","2347\n","1876\n","2365\n","2981\n","4603\n","1381\n","3116\n","867\n","4656\n","4722\n","1635\n","4594\n","4684\n","4660\n","679\n","3612\n","4409\n","3484\n","1713\n","1382\n","1108\n","1785\n","3494\n","1663\n","4809\n","4277\n","1710\n","4808\n","912\n","2568\n","1588\n","2631\n","4506\n","1711\n","1349\n","857\n","3153\n","2843\n","458\n","4606\n","3827\n","4064\n","2462\n","1943\n","3658\n","1269\n","2316\n","723\n","2415\n","1690\n","2310\n","480\n","2066\n","1600\n","204\n","2881\n","577\n","4465\n","983\n","110\n","1932\n","2630\n","3296\n","865\n","2413\n","1181\n","4372\n","4611\n","935\n","4739\n","25\n","1431\n","450\n","765\n","3544\n","2830\n","3199\n","3899\n","540\n","3732\n","304\n","4014\n","2824\n","1232\n","1054\n","3100\n","1512\n","3995\n","2518\n","433\n","506\n","724\n","677\n","1173\n","2524\n","2565\n","3366\n","2353\n","2235\n","3213\n","1206\n","2823\n","1805\n","953\n","646\n","2653\n","642\n","2540\n","1031\n","1705\n","4290\n","2826\n","4176\n","4572\n","336\n","4179\n","49\n","4695\n","1650\n","4222\n","4191\n","1300\n","4300\n","2992\n","319\n","4256\n","721\n","3952\n","3664\n","1441\n","1127\n","2354\n","3165\n","885\n","2210\n","2129\n","495\n","3049\n","3523\n","164\n","4035\n","1594\n","4391\n","1510\n","362\n","1258\n","2146\n","3275\n","2731\n","3465\n","521\n","682\n","1744\n","905\n","1230\n","1592\n","347\n","2026\n","3175\n","1226\n","3220\n","4551\n","1983\n","1795\n","1740\n","569\n","3424\n","3488\n","4742\n","3384\n","4319\n","3798\n","4593\n","2088\n","4651\n","3867\n","4005\n","3481\n","2186\n","1338\n","741\n","3073\n","920\n","1676\n","2086\n","3157\n","2690\n","3327\n","2954\n","4002\n","2232\n","4203\n","4746\n","2571\n","1941\n","4400\n","3167\n","3635\n","4252\n","3268\n","4040\n","3679\n","4715\n","812\n","1560\n","3642\n","715\n","4834\n","3598\n","3513\n","4883\n","473\n","4425\n","3236\n","1631\n","2022\n","47\n","3324\n","4139\n","1872\n","2527\n","1525\n","2753\n","4084\n","2492\n","4326\n","302\n","316\n","1224\n","3493\n","4856\n","381\n","2786\n","1484\n","1417\n","593\n","2716\n","3767\n","215\n","1687\n","3539\n","674\n","1628\n","273\n","1001\n","2650\n","3261\n","3532\n","1603\n","3154\n","2328\n","3720\n","2309\n","1559\n","4460\n","1537\n","4777\n","4387\n","3609\n","3045\n","228\n","1067\n","1745\n","3027\n","1532\n","4836\n","3182\n","3946\n","1665\n","171\n","1197\n","294\n","4024\n","1386\n","4392\n","2161\n","3198\n","3843\n","2774\n","4618\n","1256\n","4752\n","908\n","842\n","3537\n","2475\n","2351\n","4091\n","2042\n","1017\n","1757\n","3996\n","3169\n","137\n","2330\n","1156\n","1542\n","3575\n","4190\n","4859\n","3419\n","1022\n","4592\n","685\n","1187\n","1270\n","4850\n","1918\n","2036\n","2525\n","2948\n","4848\n","4220\n","3293\n","639\n","698\n","962\n","2863\n","883\n","3042\n","1211\n","4448\n","3017\n","1563\n","2114\n","1094\n","4077\n","3081\n","1348\n","2807\n","1427\n","1945\n","1674\n","2661\n","3823\n","1898\n","1179\n","2130\n","293\n","2862\n","4726\n","2319\n","1974\n","4888\n","2133\n","3936\n","1597\n","2423\n","84\n","296\n","2757\n","640\n","1407\n","1881\n","1295\n","3623\n","2756\n","4022\n","3179\n","1733\n","2317\n","1929\n","910\n","3752\n","2601\n","1734\n","1350\n","4146\n","735\n","2682\n","4144\n","3251\n","2062\n","2851\n","2752\n","1896\n","4481\n","4025\n","3434\n","4827\n","3849\n","4447\n","1948\n","1982\n","2680\n","82\n","1984\n","1901\n","3865\n","1446\n","3989\n","253\n","815\n","3757\n","377\n","4186\n","72\n","4078\n","1848\n","754\n","440\n","4750\n","1787\n","4331\n","1580\n","69\n","1827\n","2576\n","2288\n","2167\n","2360\n","2400\n","1298\n","814\n","4258\n","4436\n","3378\n","4467\n","3610\n","3750\n","4847\n","1220\n","4092\n","790\n","4681\n","1104\n","2329\n","140\n","1006\n","1947\n","2810\n","852\n","2053\n","4429\n","3963\n","836\n","2122\n","2470\n","3542\n","2507\n","2586\n","1995\n","3227\n","1335\n","221\n","403\n","2697\n","4011\n","3954\n","1064\n","4702\n","1726\n","3716\n","3877\n","2924\n","779\n","4843\n","2029\n","2750\n","2110\n","4612\n","4380\n","3618\n","4129\n","1373\n","971\n","3468\n","3438\n","1239\n","2453\n","2372\n","3594\n","1279\n","2896\n","511\n","4469\n","310\n","2692\n","1332\n","1098\n","1199\n","4721\n","756\n","3285\n","1990\n","1250\n","3984\n","4744\n","1760\n","397\n","103\n","1047\n","1866\n","4666\n","3502\n","4478\n","183\n","4674\n","3026\n","4800\n","796\n","4569\n","271\n","4685\n","134\n","932\n","923\n","1614\n","701\n","2872\n","2433\n","4483\n","3229\n","311\n","4442\n","949\n","838\n","2850\n","1989\n","2769\n","1037\n","2550\n","350\n","692\n","1119\n","4273\n","1841\n","2151\n","3653\n","2566\n","4475\n","3829\n","1791\n","4657\n","1649\n","2889\n","3890\n","4624\n","3855\n","1445\n","2958\n","1853\n","3793\n","36\n","3969\n","3486\n","3107\n","182\n","4348\n","315\n","2156\n","2546\n","3364\n","1718\n","967\n","2291\n","4199\n","1050\n","4670\n","3335\n","602\n","3570\n","4573\n","2663\n","578\n","4085\n","859\n","3507\n","3520\n","2200\n","1565\n","3932\n","3974\n","2596\n","4474\n","1501\n","2726\n","3528\n","3955\n","490\n","3062\n","4745\n","4637\n","383\n","1166\n","4008\n","1455\n","1177\n","1346\n","4298\n","317\n","2143\n","1899\n","3292\n","4548\n","3132\n","3603\n","2491\n","2638\n","3095\n","410\n","3444\n","2207\n","3692\n","354\n","1778\n","1751\n","732\n","2085\n","212\n","2280\n","3526\n","4549\n","3948\n","1754\n","4124\n","2529\n","4740\n","3957\n","2611\n","1124\n","2902\n","2739\n","3232\n","1409\n","2658\n","4386\n","4108\n","4889\n","4398\n","931\n","749\n","3369\n","4404\n","1838\n","2972\n","1764\n","2015\n","2324\n","1920\n","3398\n","3380\n","3557\n","594\n","2383\n","2875\n","4340\n","4381\n","2516\n","2762\n","2006\n","1679\n","4654\n","4155\n","3919\n","3771\n","3038\n","4430\n","2683\n","3689\n","4486\n","2448\n","643\n","1242\n","3097\n","3094\n","2510\n","725\n","3510\n","3257\n","7\n","572\n","4292\n","1144\n","343\n","2514\n","345\n","2649\n","4463\n","3740\n","4113\n","2887\n","3556\n","606\n","59\n","1277\n","2131\n","1914\n","4301\n","1529\n","3540\n","4165\n","4295\n","579\n","1632\n","4197\n","3361\n","1087\n","2434\n","4428\n","1176\n","3785\n","3792\n","4604\n","3709\n","4728\n","4892\n","99\n","2921\n","2888\n","2197\n","2003\n","835\n","4175\n","4634\n","266\n","2446\n","2669\n","197\n","2430\n","3158\n","3483\n","2128\n","624\n","4587\n","3766\n","2083\n","1000\n","3082\n","3821\n","3119\n","1139\n","957\n","3981\n","229\n","19\n","3351\n","736\n","2333\n","4819\n","2245\n","1625\n","3388\n","886\n","68\n","2986\n","3126\n","1044\n","238\n","4610\n","98\n","559\n","2746\n","2526\n","4449\n","3597\n","1208\n","2057\n","2623\n","3553\n","4709\n","4201\n","1706\n","1452\n","1027\n","4237\n","1171\n","4307\n","1420\n","2539\n","758\n","2255\n","525\n","497\n","2344\n","194\n","3881\n","4879\n","2051\n","4781\n","3445\n","4583\n","1935\n","2710\n","3206\n","4718\n","897\n","1604\n","1287\n","4876\n","2938\n","1561\n","112\n","2171\n","3515\n","4031\n","876\n","4731\n","1190\n","4713\n","641\n","1869\n","2307\n","3654\n","3246\n","1850\n","4106\n","3278\n","2094\n","1919\n","2473\n","1675\n","2920\n","4530\n","4366\n","476\n","2497\n","1643\n","4664\n","780\n","4323\n","3700\n","469\n","3979\n","4625\n","297\n","3637\n","71\n","4761\n","1294\n","564\n","722\n","1507\n","430\n","4443\n","292\n","4564\n","4700\n","3870\n","3763\n","449\n","1053\n","3972\n","3796\n","3123\n","4140\n","1045\n","2908\n","2125\n","3759\n","1158\n","2696\n","2777\n","4601\n","4081\n","3538\n","3701\n","3505\n","1024\n","4499\n","1467\n","986\n","1097\n","2811\n","1883\n","481\n","114\n","2099\n","2268\n","206\n","2685\n","1428\n","243\n","1737\n","3591\n","1769\n","4852\n","3148\n","3978\n","4692\n","554\n","3804\n","2897\n","1157\n","1957\n","1480\n","4696\n","3440\n","2404\n","785\n","373\n","4123\n","404\n","3727\n","57\n","2382\n","2911\n","2784\n","4884\n","4648\n","1817\n","4432\n","3859\n","3173\n","994\n","1954\n","2587\n","1059\n","326\n","63\n","4248\n","3306\n","2925\n","153\n","2584\n","1657\n","3898\n","2464\n","1599\n","4571\n","3412\n","3023\n","3554\n","2272\n","3174\n","420\n","1040\n","97\n","1502\n","2944\n","2246\n","3151\n","3218\n","2917\n","1566\n","3254\n","2013\n","2615\n","1556\n","4488\n","3916\n","370\n","2160\n","1069\n","3741\n","2775\n","978\n","2619\n","141\n","4378\n","659\n","845\n","942\n","684\n","1216\n","1355\n","1703\n","720\n","2341\n","4510\n","4367\n","3133\n","37\n","1383\n","1255\n","1032\n","4454\n","4479\n","4512\n","3622\n","2943\n","3020\n","438\n","1416\n","1081\n","3\n","792\n","2502\n","1877\n","4791\n","4841\n","3348\n","1915\n","3113\n","1449\n","2747\n","3548\n","1066\n","3131\n","4543\n","666\n","945\n","2422\n","104\n","630\n","2458\n","2499\n","4716\n","1284\n","2517\n","4128\n","2496\n","425\n","3495\n","2531\n","766\n","1440\n","2589\n","1315\n","2356\n","2599\n","2101\n","3372\n","1653\n","1782\n","1033\n","1366\n","781\n","588\n","2772\n","638\n","3211\n","3078\n","335\n","2787\n","1939\n","872\n","1952\n","3682\n","879\n","426\n","3644\n","959\n","2092\n","4871\n","566\n","2465\n","4421\n","3309\n","411\n","4812\n","2141\n","152\n","708\n","3922\n","86\n","234\n","3809\n","1571\n","4849\n","851\n","4246\n","4433\n","4039\n","719\n","3646\n","2127\n","2038\n","1263\n","4553\n","3214\n","747\n","4234\n","2191\n","1368\n","4757\n","2964\n","3999\n","1568\n","1701\n","3336\n","1539\n","3982\n","3920\n","3421\n","4160\n","4423\n","4464\n","2426\n","2994\n","1651\n","330\n","3298\n","2509\n","2403\n","644\n","4019\n","4015\n","3088\n","1683\n","813\n","3707\n","887\n","2570\n","3886\n","2205\n","1909\n","2411\n","391\n","2236\n","4174\n","655\n","3137\n","1268\n","2486\n","2545\n","3693\n","3052\n","1825\n","3290\n","2671\n","2490\n","2286\n","38\n","41\n","9\n","3287\n","2466\n","4431\n","1447\n","891\n","3109\n","3096\n","1836\n","4659\n","1424\n","562\n","44\n","3303\n","213\n","1656\n","2759\n","3134\n","4099\n","4371\n","1950\n","916\n","1003\n","3339\n","83\n","751\n","1774\n","2158\n","1436\n","1246\n","2123\n","898\n","232\n","2859\n","3545\n","2239\n","984\n","1264\n","4591\n","4804\n","109\n","3362\n","2216\n","173\n","1890\n","2313\n","2989\n","3404\n","1095\n","4422\n","3480\n","4662\n","4245\n","448\n","4349\n","2398\n","3499\n","1499\n","2363\n","2069\n","2876\n","321\n","249\n","4050\n","4322\n","1596\n","4121\n","3420\n","174\n","184\n","3098\n","4263\n","3838\n","2046\n","1942\n","1354\n","1793\n","4590\n","807\n","3640\n","3451\n","2817\n","4517\n","493\n","3208\n","3519\n","2749\n","1574\n","512\n","831\n","3474\n","2741\n","507\n","3064\n","841\n","4649\n","2776\n","1712\n","1609\n","4324\n","46\n","376\n","1442\n","1666\n","848\n","1351\n","4562\n","329\n","154\n","2821\n","3775\n","384\n","2224\n","1460\n","4710\n","4717\n","951\n","3869\n","138\n","2012\n","2281\n","1227\n","1905\n","447\n","2763\n","3704\n","2780\n","4093\n","334\n","4017\n","2335\n","1808\n","3432\n","3787\n","239\n","2352\n","3138\n","3784\n","3847\n","3546\n","1291\n","2818\n","4737\n","4265\n","678\n","1562\n","4337\n","1771\n","2554\n","1140\n","2829\n","829\n","2504\n","1329\n","2343\n","4051\n","3101\n","3921\n","3960\n","2489\n","3178\n","1698\n","2866\n","4764\n","4782\n","2705\n","1589\n","2265\n","854\n","4058\n","3621\n","2729\n","3831\n","4117\n","2390\n","2879\n","4074\n","4141\n","1039\n","55\n","4063\n","2073\n","1615\n","1089\n","4770\n","4223\n","4151\n","94\n","4029\n","492\n","2573\n","904\n","1076\n","1111\n","4338\n","2959\n","3873\n","762\n","58\n","4613\n","4363\n","4826\n","1324\n","2733\n","661\n","11\n","665\n","4658\n","2773\n","3883\n","2332\n","4036\n","4119\n","2656\n","1878\n","4699\n","2809\n","1794\n","3690\n","524\n","2306\n","3383\n","2039\n","3172\n","3733\n","62\n","2512\n","2686\n","2607\n","3961\n","4018\n","4098\n","977\n","3951\n","2005\n","3181\n","1020\n","752\n","3748\n","4335\n","2927\n","263\n","811\n","3367\n","543\n","1456\n","3628\n","1426\n","2297\n","1831\n","1132\n","4579\n","3162\n","2228\n","2860\n","2296\n","4607\n","1616\n","4589\n","2597\n","1931\n","4343\n","4143\n","3527\n","3376\n","4233\n","2266\n","4895\n","1662\n","1961\n","224\n","1717\n","2285\n","3193\n","1685\n","1842\n","2311\n","3879\n","2961\n","3312\n","3617\n","4896\n","4864\n","1021\n","3294\n","1213\n","1240\n","1917\n","3259\n","2969\n","1052\n","1497\n","1871\n","4558\n","1077\n","459\n","1985\n","2660\n","130\n","4276\n","2997\n","2846\n","1384\n","1535\n","922\n","1344\n","4364\n","2616\n","2940\n","1392\n","4642\n","2670\n","3410\n","3734\n","4350\n","3888\n","2935\n","993\n","214\n","1150\n","2538\n","4832\n","4661\n","582\n","2755\n","2789\n","4305\n","607\n","2115\n","3893\n","2427\n","115\n","2162\n","4501\n","3018\n","2196\n","1437\n","4462\n","3090\n","4242\n","2560\n","3673\n","1201\n","301\n","4671\n","2577\n","3797\n","1225\n","800\n","1241\n","1601\n","1473\n","4621\n","808\n","4034\n","2737\n","4181\n","451\n","4533\n","2361\n","1364\n","4678\n","948\n","2659\n","3069\n","3677\n","457\n","4802\n","3854\n","3341\n","3308\n","743\n","963\n","3620\n","3328\n","2919\n","3305\n","1851\n","3746\n","1602\n","2322\n","3252\n","1953\n","3592\n","4172\n","259\n","3695\n","2951\n","1490\n","2868\n","1595\n","4795\n","3087\n","1004\n","2595\n","956\n","3452\n","413\n","4753\n","2001\n","4796\n","2477\n","4532\n","306\n","3319\n","2442\n","4652\n","750\n","2758\n","3037\n","2633\n","3060\n","4043\n","509\n","4689\n","2647\n","2184\n","1138\n","3675\n","1514\n","2019\n","3032\n","1585\n","1339\n","827\n","3489\n","312\n","3669\n","3423\n","4765\n","4147\n","1976\n","1093\n","475\n","3783\n","167\n","483\n","3938\n","4033\n","1607\n","3751\n","21\n","2559\n","444\n","3353\n","3463\n","3860\n","2783\n","793\n","694\n","706\n","3627\n","276\n","3343\n","4227\n","2557\n","3516\n","2950\n","3813\n","2367\n","538\n","2262\n","4330\n","3265\n","518\n","1664\n","327\n","707\n","1880\n","2816\n","2867\n","4061\n","2701\n","3975\n","4797\n","4006\n","4547\n","4675\n","1134\n","4294\n","1005\n","1849\n","1153\n","4296\n","4706\n","2513\n","4476\n","1548\n","2215\n","3144\n","2406\n","4384\n","3091\n","4038\n","1839\n","1765\n","89\n","3943\n","4238\n","4385\n","860\n","199\n","1236\n","3390\n","1286\n","1293\n","1463\n","3491\n","4538\n","3159\n","3136\n","3170\n","2500\n","1113\n","2679\n","3399\n","3442\n","2107\n","2040\n","3239\n","3168\n","2878\n","3697\n","4844\n","2966\n","1986\n","1205\n","2393\n","3808\n","1694\n","1457\n","2218\n","4193\n","1167\n","2628\n","615\n","1958\n","3558\n","738\n","2211\n","1531\n","3403\n","1786\n","3102\n","3950\n","1966\n","1852\n","4115\n","4208\n","2179\n","3998\n","3129\n","3185\n","3389\n","3395\n","2634\n","2681\n","1136\n","2405\n","1688\n","2833\n","4010\n","1114\n","2806\n","3864\n","318\n","4317\n","3454\n","360\n","4755\n","2202\n","4393\n","3190\n","3417\n","503\n","4351\n","4437\n","3277\n","2379\n","1489\n","3464\n","85\n","45\n","2855\n","3397\n","186\n","2391\n","1630\n","3723\n","519\n","54\n","4308\n","4229\n","3656\n","4831\n","2508\n","4304\n","170\n","3512\n","3599\n","1247\n","489\n","2233\n","2140\n","818\n","2234\n","657\n","124\n","1314\n","3184\n","3811\n","1101\n","4126\n","3742\n","2002\n","2678\n","709\n","3204\n","514\n","3789\n","2394\n","914\n","4069\n","257\n","120\n","2590\n","3207\n","1418\n","101\n","242\n","847\n","877\n","1492\n","4097\n","1720\n","1435\n","4120\n","840\n","595\n","1533\n","2409\n","3574\n","4738\n","121\n","2240\n","4655\n","985\n","675\n","3944\n","1376\n","1110\n","619\n","4798\n","2410\n","2665\n","1443\n","2637\n","2137\n","4213\n","3884\n","1550\n","2008\n","658\n","2045\n","3085\n","1534\n","486\n","3994\n","3280\n","3321\n","2676\n","280\n","4196\n","1759\n","3355\n","74\n","4568\n","1233\n","1162\n","3103\n","357\n","460\n","1505\n","2082\n","3237\n","4771\n","3083\n","3648\n","3264\n","4633\n","1434\n","3449\n","2578\n","168\n","230\n","200\n","2481\n","1780\n","1\n","211\n","4667\n","3460\n","2076\n","3504\n","601\n","1843\n","193\n","3002\n","4484\n","4118\n","16\n","2765\n","111\n","3739\n","4595\n","2025\n","4665\n","1885\n","325\n","822\n","4154\n","4230\n","2287\n","791\n","390\n","470\n","363\n","1708\n","342\n","1552\n","2949\n","4680\n","1265\n","217\n","1218\n","2792\n","2564\n","3422\n","3416\n","4730\n","2984\n","2063\n","2334\n","2877\n","1819\n","4698\n","1261\n","2172\n","1325\n","1978\n","2102\n","17\n","2153\n","3191\n","2164\n","4228\n","1573\n","1922\n","952\n","3745\n","810\n","2237\n","185\n","832\n","2724\n","4866\n","3913\n","1345\n","1567\n","1495\n","772\n","2894\n","3450\n","548\n","1365\n","3030\n","262\n","784\n","4588\n","3340\n","3379\n","42\n","508\n","4365\n","1009\n","4874\n","233\n","3786\n","1248\n","871\n","2301\n","93\n","2467\n","798\n","4333\n","3084\n","2721\n","1030\n","437\n","3248\n","3638\n","4162\n","3258\n","4408\n","303\n","1654\n","119\n","4082\n","3749\n","530\n","2939\n","180\n","2260\n","1684\n","4578\n","1913\n","1458\n","423\n","80\n","3826\n","3013\n","2357\n","858\n","431\n","1470\n","1627\n","4457\n","1413\n","2642\n","1949\n","3768\n","4030\n","3033\n","1965\n","695\n","3874\n","1026\n","2698\n","907\n","3130\n","3663\n","3824\n","4616\n","3755\n","1579\n","1668\n","4516\n","2960\n","3935\n","3127\n","2271\n","2388\n","3729\n","4388\n","1779\n","2150\n","201\n","2535\n","668\n","3230\n","4185\n","4758\n","3678\n","1691\n","2740\n","289\n","4541\n","198\n","2718\n","1804\n","4067\n","3243\n","3586\n","3641\n","2149\n","1977\n","1969\n","3077\n","1506\n","4775\n","4806\n","3070\n","1987\n","3724\n","4829\n","954\n","1195\n","1478\n","3971\n","2743\n","541\n","1060\n","2985\n","2349\n","3910\n","3330\n","1193\n","693\n","1103\n","3660\n","4357\n","2109\n","4374\n","1845\n","4250\n","3370\n","1200\n","1993\n","1686\n","4316\n","359\n","2450\n","926\n","2998\n","633\n","2803\n","4792\n","4122\n","1168\n","1375\n","129\n","4412\n","4846\n","2503\n","4116\n","1212\n","467\n","255\n","4523\n","551\n","2515\n","597\n","2290\n","2976\n","3871\n","2982\n","2978\n","2886\n","4556\n","4751\n","794\n","1680\n","3028\n","930\n","176\n","4837\n","3993\n","4279\n","673\n","592\n","4760\n","2916\n","3858\n","126\n","1282\n","3917\n","3281\n","1118\n","3997\n","3076\n","686\n","254\n","3433\n","2050\n","3915\n","4254\n","2293\n","3310\n","2460\n","2299\n","803\n","1799\n","3326\n","1323\n","1513\n","1203\n","2767\n","3061\n","2254\n","2326\n","1946\n","1998\n","1981\n","2771\n","4041\n","2709\n","599\n","3511\n","1858\n","1016\n","527\n","3676\n","4302\n","2973\n","3777\n","2505\n","12\n","3142\n","1504\n","550\n","3718\n","1742\n","4561\n","2928\n","3375\n","2080\n","4167\n","882\n","3587\n","2779\n","4773\n","179\n","1623\n","3373\n","4133\n","4627\n","415\n","3630\n","728\n","2384\n","3983\n","313\n","4052\n","2555\n","611\n","113\n","727\n","4527\n","4559\n","39\n","1152\n","3629\n","3754\n","502\n","2275\n","2068\n","4622\n","2796\n","446\n","2610\n","830\n","2095\n","3186\n","1538\n","4153\n","2722\n","3074\n","3680\n","505\n","1130\n","2620\n","462\n","960\n","2416\n","2890\n","3806\n","474\n","4219\n","691\n","1389\n","2345\n","187\n","1450\n","3905\n","4000\n","3643\n","3188\n","3435\n","4402\n","4261\n","4026\n","4257\n","1018\n","2139\n","3019\n","2751\n","4835\n","3835\n","1257\n","2111\n","245\n","3866\n","3925\n","2369\n","2091\n","3007\n","3479\n","4783\n","4801\n","1023\n","4485\n","1530\n","2447\n","4239\n","2688\n","4231\n","1645\n","2632\n","3317\n","3634\n","4243\n","3325\n","961\n","2918\n","603\n","1999\n","192\n","4111\n","2704\n","3267\n","574\n","32\n","1228\n","2079\n","4582\n","4032\n","1010\n","1429\n","1316\n","4455\n","973\n","821\n","4221\n","4416\n","4359\n","2592\n","1126\n","1137\n","1234\n","1481\n","323\n","823\n","3408\n","2387\n","4048\n","589\n","4284\n","4458\n","3203\n","395\n","789\n","1578\n","774\n","2736\n","4707\n","349\n","3525\n","2047\n","2419\n","4102\n","4206\n","3161\n","3897\n","3283\n","3040\n","160\n","2624\n","2052\n","1639\n","1074\n","917\n","955\n","3201\n","1971\n","3238\n","4640\n","581\n","1775\n","987\n","1832\n","4646\n","1299\n","4168\n","2553\n","4310\n","358\n","2435\n","4399\n","634\n","759\n","3437\n","3271\n","1393\n","3902\n","4650\n","2078\n","929\n","3318\n","3885\n","3816\n","3080\n","2542\n","1415\n","3472\n","1959\n","2580\n","4522\n","2646\n","3458\n","2399\n","3583\n","269\n","1519\n","760\n","1617\n","18\n","4489\n","3413\n","92\n","1062\n","1923\n","4107\n","2675\n","2521\n","757\n","3425\n","2955\n","1692\n","1155\n","746\n","1151\n","287\n","2182\n","2456\n","2117\n","2279\n","4803\n","2561\n","286\n","471\n","1697\n","1170\n","1046\n","2904\n","4828\n","1019\n","2259\n","3830\n","2145\n","196\n","1541\n","2853\n","4355\n","968\n","647\n","4281\n","2606\n","498\n","604\n","1732\n","4668\n","1964\n","875\n","2227\n","1956\n","1619\n","2060\n","3615\n","3970\n","4563\n","3108\n","586\n","3762\n","801\n","2368\n","3550\n","76\n","4225\n","6\n","2222\n","3652\n","3649\n","144\n","2201\n","264\n","3577\n","4232\n","3853\n","2662\n","4647\n","1719\n","3756\n","1347\n","3359\n","3066\n","1644\n","4204\n","943\n","3005\n","2331\n","1041\n","3475\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-bu0kAaZvELN"},"source":["y_pred"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ERxUfT_EaTe1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615054186963,"user_tz":-60,"elapsed":436,"user":{"displayName":"k ww","photoUrl":"","userId":"09134966690380548520"}},"outputId":"d6e9a405-1a1f-4f7a-f04d-1ec854208e96"},"source":["from sklearn.tree import DecisionTreeClassifier\r\n","\r\n","\r\n","dt = DecisionTreeClassifier(max_depth=10)\r\n","dt.fit(xtrain,ytrain)\r\n"],"execution_count":141,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n","                       max_depth=10, max_features=None, max_leaf_nodes=None,\n","                       min_impurity_decrease=0.0, min_impurity_split=None,\n","                       min_samples_leaf=1, min_samples_split=2,\n","                       min_weight_fraction_leaf=0.0, presort='deprecated',\n","                       random_state=None, splitter='best')"]},"metadata":{"tags":[]},"execution_count":141}]},{"cell_type":"code","metadata":{"id":"Sj7KPSGWdQ_E","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615054190013,"user_tz":-60,"elapsed":440,"user":{"displayName":"k ww","photoUrl":"","userId":"09134966690380548520"}},"outputId":"36440c4f-f800-4ce4-a6de-10330abf01d1"},"source":["y_pred_sklearn = dt.predict(xtrain)\r\n","skacc = Metrics.accuracy(y_pred_sklearn,ytrain)\r\n","myacc = Metrics.accuracy(y_pred,ytrain)\r\n","\r\n","print(skacc,myacc)"],"execution_count":142,"outputs":[{"output_type":"stream","text":["74.32905484247374 74.38739789964994\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HO0TllIHdUnm"},"source":[""],"execution_count":null,"outputs":[]}]}