{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DTRegressor_Tutorial.ipynb","provenance":[],"authorship_tag":"ABX9TyMzzSxPFpjpwcXJrHRzInxu"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"Q2Ln-0DfIksI","executionInfo":{"status":"ok","timestamp":1615044201979,"user_tz":-60,"elapsed":589,"user":{"displayName":"k ww","photoUrl":"","userId":"09134966690380548520"}}},"source":["import numpy as np\r\n","import matplotlib.pyplot as plt"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"EJWcvVWYJCWC"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7-5Z-sMiIrjP"},"source":["class DTRegressor():\r\n","  @staticmethoddef mse(v):\r\n","  return np.mean(np.square(v-np.mean(v)))\r\n","\r\n","  @staticmethod\r\n","  def split_data(X, y, feature_index, feature_value):\r\n","    \"\"\"Only Consider numerical condition\"\"\"\r\n","    return {\r\n","      \"I_left\": np.where(X[:, feature_index] <= feature_value)[0],\r\n","      \"I_right\": np.where(X[:, feature_index] > feature_value)[0],\r\n","    }\r\n","\r\n","  @staticmethod\r\n","  def greedy_best_split(X, y):\r\n","    best_feature_index = 0\r\n","    best_split_value = 0\r\n","    best_dloss = 0\r\n","    best_split = {\r\n","      \"I_left\": np.array([]),\r\n","      \"I_right\": np.array([]),\r\n","    }\r\n","\r\n","    n_features = X.shape[1]\r\n","    parent_mse = RegTree.mse(y)\r\n","    N = y.shape[0]\r\n","    for feature_index in range(0, n_features):\r\n","      split_values = np.unique(X[:, feature_index])\r\n","      for split_value in split_values:\r\n","        split = RegTree.split_data(X, y, feature_index, split_value)\r\n","\r\n","        # If there is a split\r\n","        if split[\"I_left\"].shape[0] > 0 and split[\"I_right\"].shape[0] > 0:\r\n","          # Compute the change in loss\r\n","          N_left = split[\"I_left\"].shape[0]\r\n","          N_right = split[\"I_right\"].shape[0]\r\n","          dloss = parent_mse - 1/N * (N_left * RegTree.mse(y[split[\"I_left\"]]) + N_right * RegTree.mse(y[split[\"I_right\"]]))\r\n","\r\n","          # Update if the change in loss is the largest so far\r\n","          if dloss >= best_dloss:\r\n","            best_feature_index = feature_index\r\n","            best_split_value = split_value\r\n","            best_split = split\r\n","            best_dloss = dloss\r\n","\r\n","    return best_dloss, best_feature_index, best_split_value, best_split\r\n","\r\n","  @staticmethod\r\n","  def fit_tree(X, y, depth = 1, \r\n","               max_depth = 100, tolerance = 10**(-3)):\r\n","    node = {}\r\n","\r\n","    # Predict with the mean\r\n","    node[\"w\"] = np.mean(y)\r\n","\r\n","    node[\"left\"] = None\r\n","    node[\"right\"] = None\r\n","\r\n","    # If we can split, find the best split by greedy algorithm\r\n","    if y.shape[0] >= 2:\r\n","      dloss, feature_index, split_value, split = RegTree.greedy_best_split(X, y)\r\n","      # If there is a greedy split and the stopping criterion is not met, branch 2 times\r\n","      if split[\"I_left\"].shape[0] > 0 and split[\"I_right\"].shape[0] > 0 and dloss >= tolerance and depth < max_depth:\r\n","        node[\"dloss\"] = dloss\r\n","        node[\"feature_index\"] = feature_index\r\n","        node[\"split_value\"] = split_value\r\n","\r\n","        node[\"left\"] = RegTree.fit_tree(X[split[\"I_left\"]], y[split[\"I_left\"]], depth = depth + 1, max_depth = max_depth, tolerance = tolerance)\r\n","        node[\"right\"] = RegTree.fit_tree(X[split[\"I_right\"]], y[split[\"I_right\"]], depth = depth + 1, max_depth = max_depth, tolerance = tolerance) \r\n","    return node\r\n","\r\n","  @staticmethod\r\n","  def predict_one(node, x):\r\n","    if node[\"left\"] == None:\r\n","      return node[\"w\"]\r\n","    else:\r\n","      if x[node[\"feature_index\"]] <= node[\"split_value\"]:\r\n","        return RegTree.predict_one(node[\"left\"], x)\r\n","      else:\r\n","        return RegTree.predict_one(node[\"right\"], x)\r\n","\r\n","  @staticmethod\r\n","  def predict(node, X):\r\n","    n_samples = X.shape[0]\r\n","    predictions = np.zeros(n_samples)\r\n","    for i in range(0, n_samples):\r\n","      predictions[i] = RegTree.predict_one(node, X[i])\r\n","    return predictions"],"execution_count":null,"outputs":[]}]}