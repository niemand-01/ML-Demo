{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Hyperparameter-tuning-pytorch-RayTune.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNehuLUIiz4QKtORHxXxlZg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niemand-01/ML-Demo/blob/master/Hyperparameter_tuning_pytorch_RayTune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZhej4eVesXk"
      },
      "source": [
        "from functools import partial\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import random_split\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "!pip install ray\n",
        "from ray import tune\n",
        "from ray.tune import CLIReporter\n",
        "from ray.tune.schedulers import ASHAScheduler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78LkcC5AfCVJ"
      },
      "source": [
        "# load data\n",
        "\n",
        "def load_data(data_dir=\"./data\"):\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    trainset = torchvision.datasets.CIFAR10(\n",
        "        root=data_dir, train=True, download=True, transform=transform)\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR10(\n",
        "        root=data_dir, train=False, download=True, transform=transform)\n",
        "\n",
        "    return trainset, testset"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1fcOuv_fV2X"
      },
      "source": [
        "# simple NN with 2 tunable parameters:\n",
        "# l1,l2 at FC2,FC3\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, l1=120, l2=84):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, l1)\n",
        "        self.fc2 = nn.Linear(l1, l2)\n",
        "        self.fc3 = nn.Linear(l2, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 16 * 5 * 5)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wm05onajfobx"
      },
      "source": [
        "def train(config,checkpoint_dir=None,data_dir=None):\n",
        "  # config func is a python-built-in func for exposed params configuration\n",
        "  net = Net(config[\"l1\"],config[\"l2\"])\n",
        "\n",
        "  # optimizer\n",
        "  optimizer = optim.SGD(net.parameters(),lr=config[\"lr\"],momentum=0.9)\n",
        "  # loss\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "  # device\n",
        "  device = \"cpu\"\n",
        "\n",
        "  # save checkpoint \n",
        "  if checkpoint_dir:\n",
        "    model_state,optimizer_state = torch.load(\n",
        "        os.path.join(checkpoint_dir,\"checkpoint\")\n",
        "    )\n",
        "    net.load_state_dict(model_state)\n",
        "    optimizer.load_state_dict(optimizer_state)\n",
        "\n",
        "  # divide train/validation set = 80/20\n",
        "  trainset,testset = load_data(data_dir)\n",
        "  test_abs = int(len(trainset)*0.8)\n",
        "  # split 20% of trainset to validationset\n",
        "  train_subset,val_subset = random_split(\n",
        "      trainset,[test_abs,len(trainset)-test_abs]\n",
        "  )\n",
        "\n",
        "  # load generator\n",
        "  trainloader = torch.utils.data.DataLoader(\n",
        "      train_subset,\n",
        "      batch_size = int(config[\"batch_size\"]),\n",
        "      shuffle = True,\n",
        "      num_workers = 8\n",
        "  )\n",
        "  validloader = torch.utils.data.DataLoader(\n",
        "      val_subset,\n",
        "      batch_size = int(config[\"batch_size\"]),\n",
        "      shuffle = True,\n",
        "      num_workers = 8\n",
        "  )\n",
        "\n",
        "  for epoch in range(10):\n",
        "    running_loss = 0.0\n",
        "    epoch_steps = 0\n",
        "\n",
        "    # each epoch traindata\n",
        "    for i, data in enumerate(trainloader,0):\n",
        "      # get the inputs; data is a list of [inputs, labels]\n",
        "      inputs, labels = data\n",
        "      inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "      # zero the parameter gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # forward + backward + optimize\n",
        "      outputs = net(inputs)\n",
        "      loss = criterion(outputs, labels)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # print statistics\n",
        "      running_loss += loss.item()\n",
        "      epoch_steps += 1\n",
        "      if i % 2000 == 1999:  \n",
        "        # print every 2000 mini-batches\n",
        "        print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,running_loss / epoch_steps))\n",
        "        running_loss = 0.0\n",
        "\n",
        "    # each epoch validdata\n",
        "    # Validation loss\n",
        "    val_loss = 0.0\n",
        "    val_steps = 0\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for i, data in enumerate(validloader, 0):\n",
        "      # disable gradient calculation\n",
        "      with torch.no_grad():\n",
        "        inputs, labels = data\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = net(inputs)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        val_loss += loss.cpu().numpy()\n",
        "        val_steps += 1\n",
        "\n",
        "\n",
        "    # communication with raytune\n",
        "    with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
        "      path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
        "      torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
        "\n",
        "    tune.report(loss=(val_loss / val_steps), accuracy=correct / total)\n",
        "    print(\"Finished Training\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOlAXazEjzov"
      },
      "source": [
        "# configure the search space\n",
        "\n",
        "config = {\n",
        "    \"l1\":tune.sample_from(lambda _: 2**np.random.randint(2,9)),\n",
        "    \"l2\":tune.sample_from(lambda _:2**np.random.randint(2,9)),\n",
        "    \"lr\":tune.loguniform(1e-4,1e-1),\n",
        "    \"batch_size\": tune.choice([2,4,8,16])\n",
        "}"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-1zZmFqkqLi"
      },
      "source": [
        "# main function\n",
        "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=0):\n",
        "    data_dir = os.path.abspath(\"./data\")\n",
        "    load_data(data_dir)\n",
        "    config = {\n",
        "        \"l1\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
        "        \"l2\": tune.sample_from(lambda _: 2 ** np.random.randint(2, 9)),\n",
        "        \"lr\": tune.loguniform(1e-4, 1e-1),\n",
        "        \"batch_size\": tune.choice([2, 4, 8, 16])\n",
        "    }\n",
        "    # A scheduler decides which trials to run, stop, or pause\n",
        "    scheduler = ASHAScheduler(\n",
        "        metric=\"loss\",\n",
        "        mode=\"min\",\n",
        "        max_t=max_num_epochs,\n",
        "        grace_period=1,\n",
        "        reduction_factor=2)\n",
        "    # print progress\n",
        "    reporter = CLIReporter(\n",
        "        # parameter_columns=[\"l1\", \"l2\", \"lr\", \"batch_size\"],\n",
        "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
        "    \n",
        "    # run tuning\n",
        "    result = tune.run(\n",
        "        partial(train, data_dir=data_dir),\n",
        "        resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial},\n",
        "        config=config,\n",
        "        num_samples=num_samples,\n",
        "        scheduler=scheduler,\n",
        "        progress_reporter=reporter)\n",
        "\n",
        "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
        "    print(\"Best trial config: {}\".format(best_trial.config))\n",
        "    print(\"Best trial final validation loss: {}\".format(\n",
        "        best_trial.last_result[\"loss\"]))\n",
        "    print(\"Best trial final validation accuracy: {}\".format(\n",
        "        best_trial.last_result[\"accuracy\"]))\n",
        "\n",
        "    best_trained_model = Net(best_trial.config[\"l1\"], best_trial.config[\"l2\"])\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda:0\"\n",
        "        if gpus_per_trial > 1:\n",
        "            best_trained_model = nn.DataParallel(best_trained_model)\n",
        "    best_trained_model.to(device)\n",
        "\n",
        "    best_checkpoint_dir = best_trial.checkpoint.value\n",
        "    model_state, optimizer_state = torch.load(os.path.join(\n",
        "        best_checkpoint_dir, \"checkpoint\"))\n",
        "    best_trained_model.load_state_dict(model_state)\n",
        "\n",
        "    test_acc = test_accuracy(best_trained_model, device)\n",
        "    print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # You can change the number of GPUs per trial here:\n",
        "    main(num_samples=10, max_num_epochs=10, gpus_per_trial=0)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}