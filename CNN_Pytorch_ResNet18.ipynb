{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN-Pytorch-ResNet18.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMXG9lvJSpVuL79dKSBZOEW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/niemand-01/ML-Demo/blob/master/CNN_Pytorch_ResNet18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-g2vbVb4SLx"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import transforms\n",
        "import torchvision\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izsL4lpQ6nBb"
      },
      "source": [
        "# device configuration\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# hyper parameters\n",
        "Epochs = 20\n",
        "batch_size = 64\n",
        "learning_rate =0.001\n",
        "classes = 10"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eGnv4mQ61pv",
        "outputId": "5ca57014-e1f9-4f5c-e5b4-b517ad345146",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "# image preprocessing modules\n",
        "transform = transforms.Compose([\n",
        "                #先padding=4\n",
        "                transforms.Pad(4),\n",
        "                #图像一半的概率翻转，一半的概率不翻转\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                #把图像随机裁剪成32*32\n",
        "                transforms.RandomCrop(32),\n",
        "                transforms.ToTensor(),])\n",
        "\n",
        "# cifar10 32*32*3\n",
        "train_dataset = torchvision.datasets.CIFAR10(root=\"./cifar10_data\",train=True,transform=transform,download=True)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root=\"./cifar10_data\",train=False,transform=transforms.ToTensor(),download=True)\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)\n",
        "testloader = torch.utils.data.DataLoader(dataset=test_dataset,batch_size=batch_size,shuffle=False)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLKpOXPaDFF6"
      },
      "source": [
        "# Single Residual Blcok\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6xhYEmA-96W",
        "outputId": "5bde7459-1e66-48f4-cace-169c7d1c8802",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        "from IPython.display import Image\n",
        "Image(url=\"https://img-blog.csdn.net/2018042621572485?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1bnFpYW5kZTg4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\")"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://img-blog.csdn.net/2018042621572485?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1bnFpYW5kZTg4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XixPOowE8NWt"
      },
      "source": [
        "# residualBlock 继承nn.module类\n",
        "class ResidualBlock(nn.Module):\n",
        "  def __init__(self,in_channels,out_channels,stride=1):\n",
        "    super().__init__()\n",
        "    self.conv_left = nn.Sequential(\n",
        "        # downsample if stride != 1\n",
        "        nn.Conv2d(in_channels,out_channels,3,stride,padding=1,bias=False),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True),\n",
        "        # maintain shape\n",
        "        nn.Conv2d(out_channels,out_channels,3,stride=1,padding=1,bias=False),\n",
        "        nn.BatchNorm2d(out_channels)\n",
        "    )\n",
        "\n",
        "    self.shortcut = nn.Sequential()\n",
        "    if stride != 1 or in_channels != out_channels:\n",
        "      self.shortcut = nn.Sequential(\n",
        "          nn.Conv2d(in_channels,out_channels,3,stride=stride,padding=1,bias=False),\n",
        "          nn.BatchNorm2d(out_channels)\n",
        "      )\n",
        "\n",
        "  def forward(self,input):\n",
        "    residual = input\n",
        "    x = self.conv_left(input)\n",
        "    shortcut = self.shortcut(residual)\n",
        "    # print(r\"left shape:{},right shape:{}\".format(x.size(),shortcut.size()))\n",
        "    # residual shortcut\n",
        "    x = x + shortcut\n",
        "    output = F.relu(x)\n",
        "    return output\n"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNr0TWBLDLOS"
      },
      "source": [
        "# ResNet18 Overview\n",
        "## Remark\n",
        "1. for each ResidualBlock we have 2 layers and we build 2 Residualblocks, which means 4 layers\n",
        "2. we downsample only in first ResidualBlock and keep dim in the second Residualblock\n",
        "3. In the first Residualblock we only downsample at first layer and keep dim at the second layer\n",
        "4. shortcut is the same as input if we have same dimension at in/-output channels\n",
        "5. shortcut must also be downsampled if in/-ouput channels are different"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtY1qHawC7Rj",
        "outputId": "0fa6a7ca-56a6-4303-c6f8-b7058392cc67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "source": [
        "Image(url=\"https://img-blog.csdn.net/20180426215052446?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1bnFpYW5kZTg4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\")"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://img-blog.csdn.net/20180426215052446?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3N1bnFpYW5kZTg4/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHFkTIRB9_a-"
      },
      "source": [
        "# ResNet\n",
        "class ResNet(nn.Module):\n",
        "  def __init__(self,block,layers,num_classes=10):\n",
        "    super().__init__()\n",
        "    self.in_channels = 64\n",
        "    self.prepare_conv = nn.Sequential(\n",
        "        nn.Conv2d(3,64,3,stride=1,padding=1,bias=False),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(inplace=True),\n",
        "    )\n",
        "    self.layer1 = self.make_layer(block,64,blocks_num=2,stride=1)\n",
        "    self.layer2 = self.make_layer(block,128,blocks_num=2,stride=2)\n",
        "    self.layer3 = self.make_layer(block,256,blocks_num=2,stride=2)\n",
        "    self.layer4 = self.make_layer(block,512,blocks_num=2,stride=2) \n",
        "    self.fc = nn.Linear(512,num_classes)\n",
        "\n",
        "  def make_layer(self,block,out_channels,blocks_num=2,stride=1):\n",
        "    # first block with stride 2(downsample), second block with stride 1 maintaining image dim\n",
        "    strides = [stride] + [1] * (blocks_num-1) #[2,1] or [1,1]\n",
        "    layers = []\n",
        "    for stride in strides:\n",
        "      layers.append(block(self.in_channels,out_channels,stride))\n",
        "      self.in_channels = out_channels\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self,input):\n",
        "    x = self.prepare_conv(input)\n",
        "\n",
        "    x = self.layer1(x)\n",
        "    x = self.layer2(x)\n",
        "    x = self.layer3(x)\n",
        "    x = self.layer4(x)\n",
        "\n",
        "    x = F.avg_pool2d(x,4)\n",
        "    # tensor.view() is like reshape() with the size -1 is inferred from other dimensions\n",
        "    # e.g. (4,4) --->view(16,-1) == (16,1) --->view(8,-1) == (8,2)\n",
        "    # basically it's like x.flatten() == x.view(x.size(0),-1)\n",
        "    x = x.view(x.size(0),-1)\n",
        "    x = self.fc(x)\n",
        "    return x\n",
        "\n",
        "# define model\n",
        "model = ResNet(ResidualBlock,[2,2,2,2]).to(device)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9xL-qSiy4Ho"
      },
      "source": [
        "# model summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmd6uPzvymYT",
        "outputId": "092a3b00-9e2d-4137-e524-b0e19e95d1c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "summary(model.cpu(),input_size=(3,32,32))"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1           [-1, 64, 32, 32]           1,728\n",
            "       BatchNorm2d-2           [-1, 64, 32, 32]             128\n",
            "              ReLU-3           [-1, 64, 32, 32]               0\n",
            "            Conv2d-4           [-1, 64, 32, 32]          36,864\n",
            "       BatchNorm2d-5           [-1, 64, 32, 32]             128\n",
            "              ReLU-6           [-1, 64, 32, 32]               0\n",
            "            Conv2d-7           [-1, 64, 32, 32]          36,864\n",
            "       BatchNorm2d-8           [-1, 64, 32, 32]             128\n",
            "     ResidualBlock-9           [-1, 64, 32, 32]               0\n",
            "           Conv2d-10           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-11           [-1, 64, 32, 32]             128\n",
            "             ReLU-12           [-1, 64, 32, 32]               0\n",
            "           Conv2d-13           [-1, 64, 32, 32]          36,864\n",
            "      BatchNorm2d-14           [-1, 64, 32, 32]             128\n",
            "    ResidualBlock-15           [-1, 64, 32, 32]               0\n",
            "           Conv2d-16          [-1, 128, 16, 16]          73,728\n",
            "      BatchNorm2d-17          [-1, 128, 16, 16]             256\n",
            "             ReLU-18          [-1, 128, 16, 16]               0\n",
            "           Conv2d-19          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-20          [-1, 128, 16, 16]             256\n",
            "           Conv2d-21          [-1, 128, 16, 16]          73,728\n",
            "      BatchNorm2d-22          [-1, 128, 16, 16]             256\n",
            "    ResidualBlock-23          [-1, 128, 16, 16]               0\n",
            "           Conv2d-24          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-25          [-1, 128, 16, 16]             256\n",
            "             ReLU-26          [-1, 128, 16, 16]               0\n",
            "           Conv2d-27          [-1, 128, 16, 16]         147,456\n",
            "      BatchNorm2d-28          [-1, 128, 16, 16]             256\n",
            "    ResidualBlock-29          [-1, 128, 16, 16]               0\n",
            "           Conv2d-30            [-1, 256, 8, 8]         294,912\n",
            "      BatchNorm2d-31            [-1, 256, 8, 8]             512\n",
            "             ReLU-32            [-1, 256, 8, 8]               0\n",
            "           Conv2d-33            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-34            [-1, 256, 8, 8]             512\n",
            "           Conv2d-35            [-1, 256, 8, 8]         294,912\n",
            "      BatchNorm2d-36            [-1, 256, 8, 8]             512\n",
            "    ResidualBlock-37            [-1, 256, 8, 8]               0\n",
            "           Conv2d-38            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-39            [-1, 256, 8, 8]             512\n",
            "             ReLU-40            [-1, 256, 8, 8]               0\n",
            "           Conv2d-41            [-1, 256, 8, 8]         589,824\n",
            "      BatchNorm2d-42            [-1, 256, 8, 8]             512\n",
            "    ResidualBlock-43            [-1, 256, 8, 8]               0\n",
            "           Conv2d-44            [-1, 512, 4, 4]       1,179,648\n",
            "      BatchNorm2d-45            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-46            [-1, 512, 4, 4]               0\n",
            "           Conv2d-47            [-1, 512, 4, 4]       2,359,296\n",
            "      BatchNorm2d-48            [-1, 512, 4, 4]           1,024\n",
            "           Conv2d-49            [-1, 512, 4, 4]       1,179,648\n",
            "      BatchNorm2d-50            [-1, 512, 4, 4]           1,024\n",
            "    ResidualBlock-51            [-1, 512, 4, 4]               0\n",
            "           Conv2d-52            [-1, 512, 4, 4]       2,359,296\n",
            "      BatchNorm2d-53            [-1, 512, 4, 4]           1,024\n",
            "             ReLU-54            [-1, 512, 4, 4]               0\n",
            "           Conv2d-55            [-1, 512, 4, 4]       2,359,296\n",
            "      BatchNorm2d-56            [-1, 512, 4, 4]           1,024\n",
            "    ResidualBlock-57            [-1, 512, 4, 4]               0\n",
            "           Linear-58                   [-1, 10]           5,130\n",
            "================================================================\n",
            "Total params: 12,550,218\n",
            "Trainable params: 12,550,218\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.01\n",
            "Forward/backward pass size (MB): 13.63\n",
            "Params size (MB): 47.88\n",
            "Estimated Total Size (MB): 61.51\n",
            "----------------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_S_eCnCcA_rx",
        "outputId": "a233ddb7-3221-4f9e-bd00-ee079cf4875e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
        "\n",
        "# updating learning rate\n",
        "def update_lr(optimizer,lr):\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] = lr \n",
        "\n",
        "# train model\n",
        "per_epoch_total_step = len(trainloader)\n",
        "current_lr = learning_rate\n",
        "sum_loss = 0.0\n",
        "total_label = 0\n",
        "correct_label = 0\n",
        "for epoch in range(Epochs):\n",
        "  for i,(x,y) in enumerate(trainloader):\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    # optimizer \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # forward\n",
        "    prediction = model(x)\n",
        "    loss = criterion(prediction,y)\n",
        "\n",
        "    # backward\n",
        "    \n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # print for every batch loss\n",
        "    sum_loss += loss.item()\n",
        "    _, predicted = torch.max(prediction.data, 1)\n",
        "    total_label += y.size(0)\n",
        "    correct_label += predicted.eq(y.data).cpu().sum()\n",
        "    print(r'[epoch:{:d}, current_iter:{:d}, total iter:{:d}] Avg Loss: {:.3f} | Acc: {:.3f} '.format(epoch + 1, i, (epoch+1)*per_epoch_total_step, torch.true_divide(sum_loss,(i + 1)), 100. * torch.true_divide(correct_label,total_label)))\n",
        "\n",
        "    # print for every epoch\n",
        "    if (i+1)% 100 == 0:\n",
        "      template = r\"Epoch:{}/{},step:{}/{},Loss:{:.6f}\"\n",
        "      print(template.format(epoch+1,Epochs,i+1,per_epoch_total_step,loss.item()))\n",
        "\n",
        "    # lr decay\n",
        "    if (epoch+1) % 20 ==0:\n",
        "      current_lr = current_lr/2\n",
        "      update_lr(optimizer,current_lr)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[epoch:1, current_iter:0, total iter:782] Avg Loss: 2.125 | Acc: 20.312 \n",
            "[epoch:1, current_iter:1, total iter:782] Avg Loss: 2.228 | Acc: 18.750 \n",
            "[epoch:1, current_iter:2, total iter:782] Avg Loss: 2.104 | Acc: 21.354 \n",
            "[epoch:1, current_iter:3, total iter:782] Avg Loss: 2.116 | Acc: 19.531 \n",
            "[epoch:1, current_iter:4, total iter:782] Avg Loss: 2.161 | Acc: 18.438 \n",
            "[epoch:1, current_iter:5, total iter:782] Avg Loss: 2.165 | Acc: 17.188 \n",
            "[epoch:1, current_iter:6, total iter:782] Avg Loss: 2.134 | Acc: 18.304 \n",
            "[epoch:1, current_iter:7, total iter:782] Avg Loss: 2.154 | Acc: 18.164 \n",
            "[epoch:1, current_iter:8, total iter:782] Avg Loss: 2.166 | Acc: 18.229 \n",
            "[epoch:1, current_iter:9, total iter:782] Avg Loss: 2.140 | Acc: 20.156 \n",
            "[epoch:1, current_iter:10, total iter:782] Avg Loss: 2.147 | Acc: 19.744 \n",
            "[epoch:1, current_iter:11, total iter:782] Avg Loss: 2.125 | Acc: 20.964 \n",
            "[epoch:1, current_iter:12, total iter:782] Avg Loss: 2.117 | Acc: 21.274 \n",
            "[epoch:1, current_iter:13, total iter:782] Avg Loss: 2.114 | Acc: 21.094 \n",
            "[epoch:1, current_iter:14, total iter:782] Avg Loss: 2.118 | Acc: 20.625 \n",
            "[epoch:1, current_iter:15, total iter:782] Avg Loss: 2.111 | Acc: 20.703 \n",
            "[epoch:1, current_iter:16, total iter:782] Avg Loss: 2.101 | Acc: 20.956 \n",
            "[epoch:1, current_iter:17, total iter:782] Avg Loss: 2.101 | Acc: 20.660 \n",
            "[epoch:1, current_iter:18, total iter:782] Avg Loss: 2.097 | Acc: 21.053 \n",
            "[epoch:1, current_iter:19, total iter:782] Avg Loss: 2.083 | Acc: 21.562 \n",
            "[epoch:1, current_iter:20, total iter:782] Avg Loss: 2.076 | Acc: 21.801 \n",
            "[epoch:1, current_iter:21, total iter:782] Avg Loss: 2.071 | Acc: 22.443 \n",
            "[epoch:1, current_iter:22, total iter:782] Avg Loss: 2.064 | Acc: 22.622 \n",
            "[epoch:1, current_iter:23, total iter:782] Avg Loss: 2.053 | Acc: 22.917 \n",
            "[epoch:1, current_iter:24, total iter:782] Avg Loss: 2.046 | Acc: 23.312 \n",
            "[epoch:1, current_iter:25, total iter:782] Avg Loss: 2.038 | Acc: 23.858 \n",
            "[epoch:1, current_iter:26, total iter:782] Avg Loss: 2.028 | Acc: 24.074 \n",
            "[epoch:1, current_iter:27, total iter:782] Avg Loss: 2.018 | Acc: 23.996 \n",
            "[epoch:1, current_iter:28, total iter:782] Avg Loss: 2.012 | Acc: 24.246 \n",
            "[epoch:1, current_iter:29, total iter:782] Avg Loss: 2.004 | Acc: 24.583 \n",
            "[epoch:1, current_iter:30, total iter:782] Avg Loss: 1.995 | Acc: 24.899 \n",
            "[epoch:1, current_iter:31, total iter:782] Avg Loss: 1.988 | Acc: 24.951 \n",
            "[epoch:1, current_iter:32, total iter:782] Avg Loss: 1.988 | Acc: 25.047 \n",
            "[epoch:1, current_iter:33, total iter:782] Avg Loss: 1.985 | Acc: 25.184 \n",
            "[epoch:1, current_iter:34, total iter:782] Avg Loss: 1.988 | Acc: 25.313 \n",
            "[epoch:1, current_iter:35, total iter:782] Avg Loss: 1.986 | Acc: 25.391 \n",
            "[epoch:1, current_iter:36, total iter:782] Avg Loss: 1.985 | Acc: 25.591 \n",
            "[epoch:1, current_iter:37, total iter:782] Avg Loss: 1.980 | Acc: 25.699 \n",
            "[epoch:1, current_iter:38, total iter:782] Avg Loss: 1.973 | Acc: 25.962 \n",
            "[epoch:1, current_iter:39, total iter:782] Avg Loss: 1.967 | Acc: 26.367 \n",
            "[epoch:1, current_iter:40, total iter:782] Avg Loss: 1.968 | Acc: 26.562 \n",
            "[epoch:1, current_iter:41, total iter:782] Avg Loss: 1.966 | Acc: 26.525 \n",
            "[epoch:1, current_iter:42, total iter:782] Avg Loss: 1.962 | Acc: 26.708 \n",
            "[epoch:1, current_iter:43, total iter:782] Avg Loss: 1.962 | Acc: 26.918 \n",
            "[epoch:1, current_iter:44, total iter:782] Avg Loss: 1.959 | Acc: 27.014 \n",
            "[epoch:1, current_iter:45, total iter:782] Avg Loss: 1.955 | Acc: 27.208 \n",
            "[epoch:1, current_iter:46, total iter:782] Avg Loss: 1.956 | Acc: 27.194 \n",
            "[epoch:1, current_iter:47, total iter:782] Avg Loss: 1.953 | Acc: 27.148 \n",
            "[epoch:1, current_iter:48, total iter:782] Avg Loss: 1.956 | Acc: 26.945 \n",
            "[epoch:1, current_iter:49, total iter:782] Avg Loss: 1.954 | Acc: 26.812 \n",
            "[epoch:1, current_iter:50, total iter:782] Avg Loss: 1.952 | Acc: 26.808 \n",
            "[epoch:1, current_iter:51, total iter:782] Avg Loss: 1.950 | Acc: 26.953 \n",
            "[epoch:1, current_iter:52, total iter:782] Avg Loss: 1.952 | Acc: 26.887 \n",
            "[epoch:1, current_iter:53, total iter:782] Avg Loss: 1.947 | Acc: 27.112 \n",
            "[epoch:1, current_iter:54, total iter:782] Avg Loss: 1.943 | Acc: 27.216 \n",
            "[epoch:1, current_iter:55, total iter:782] Avg Loss: 1.943 | Acc: 27.232 \n",
            "[epoch:1, current_iter:56, total iter:782] Avg Loss: 1.938 | Acc: 27.357 \n",
            "[epoch:1, current_iter:57, total iter:782] Avg Loss: 1.936 | Acc: 27.290 \n",
            "[epoch:1, current_iter:58, total iter:782] Avg Loss: 1.933 | Acc: 27.489 \n",
            "[epoch:1, current_iter:59, total iter:782] Avg Loss: 1.930 | Acc: 27.656 \n",
            "[epoch:1, current_iter:60, total iter:782] Avg Loss: 1.926 | Acc: 27.843 \n",
            "[epoch:1, current_iter:61, total iter:782] Avg Loss: 1.923 | Acc: 27.898 \n",
            "[epoch:1, current_iter:62, total iter:782] Avg Loss: 1.919 | Acc: 28.125 \n",
            "[epoch:1, current_iter:63, total iter:782] Avg Loss: 1.920 | Acc: 28.101 \n",
            "[epoch:1, current_iter:64, total iter:782] Avg Loss: 1.919 | Acc: 28.053 \n",
            "[epoch:1, current_iter:65, total iter:782] Avg Loss: 1.917 | Acc: 28.125 \n",
            "[epoch:1, current_iter:66, total iter:782] Avg Loss: 1.914 | Acc: 28.172 \n",
            "[epoch:1, current_iter:67, total iter:782] Avg Loss: 1.909 | Acc: 28.355 \n",
            "[epoch:1, current_iter:68, total iter:782] Avg Loss: 1.906 | Acc: 28.465 \n",
            "[epoch:1, current_iter:69, total iter:782] Avg Loss: 1.904 | Acc: 28.549 \n",
            "[epoch:1, current_iter:70, total iter:782] Avg Loss: 1.900 | Acc: 28.807 \n",
            "[epoch:1, current_iter:71, total iter:782] Avg Loss: 1.899 | Acc: 28.819 \n",
            "[epoch:1, current_iter:72, total iter:782] Avg Loss: 1.897 | Acc: 28.917 \n",
            "[epoch:1, current_iter:73, total iter:782] Avg Loss: 1.894 | Acc: 28.927 \n",
            "[epoch:1, current_iter:74, total iter:782] Avg Loss: 1.894 | Acc: 29.000 \n",
            "[epoch:1, current_iter:75, total iter:782] Avg Loss: 1.890 | Acc: 29.091 \n",
            "[epoch:1, current_iter:76, total iter:782] Avg Loss: 1.888 | Acc: 29.200 \n",
            "[epoch:1, current_iter:77, total iter:782] Avg Loss: 1.888 | Acc: 29.187 \n",
            "[epoch:1, current_iter:78, total iter:782] Avg Loss: 1.886 | Acc: 29.312 \n",
            "[epoch:1, current_iter:79, total iter:782] Avg Loss: 1.884 | Acc: 29.492 \n",
            "[epoch:1, current_iter:80, total iter:782] Avg Loss: 1.883 | Acc: 29.475 \n",
            "[epoch:1, current_iter:81, total iter:782] Avg Loss: 1.882 | Acc: 29.440 \n",
            "[epoch:1, current_iter:82, total iter:782] Avg Loss: 1.879 | Acc: 29.612 \n",
            "[epoch:1, current_iter:83, total iter:782] Avg Loss: 1.879 | Acc: 29.632 \n",
            "[epoch:1, current_iter:84, total iter:782] Avg Loss: 1.879 | Acc: 29.743 \n",
            "[epoch:1, current_iter:85, total iter:782] Avg Loss: 1.881 | Acc: 29.724 \n",
            "[epoch:1, current_iter:86, total iter:782] Avg Loss: 1.879 | Acc: 29.795 \n",
            "[epoch:1, current_iter:87, total iter:782] Avg Loss: 1.878 | Acc: 29.901 \n",
            "[epoch:1, current_iter:88, total iter:782] Avg Loss: 1.876 | Acc: 29.951 \n",
            "[epoch:1, current_iter:89, total iter:782] Avg Loss: 1.873 | Acc: 30.017 \n",
            "[epoch:1, current_iter:90, total iter:782] Avg Loss: 1.872 | Acc: 29.962 \n",
            "[epoch:1, current_iter:91, total iter:782] Avg Loss: 1.868 | Acc: 30.078 \n",
            "[epoch:1, current_iter:92, total iter:782] Avg Loss: 1.871 | Acc: 29.990 \n",
            "[epoch:1, current_iter:93, total iter:782] Avg Loss: 1.872 | Acc: 29.953 \n",
            "[epoch:1, current_iter:94, total iter:782] Avg Loss: 1.869 | Acc: 30.000 \n",
            "[epoch:1, current_iter:95, total iter:782] Avg Loss: 1.871 | Acc: 29.997 \n",
            "[epoch:1, current_iter:96, total iter:782] Avg Loss: 1.870 | Acc: 29.994 \n",
            "[epoch:1, current_iter:97, total iter:782] Avg Loss: 1.869 | Acc: 29.990 \n",
            "[epoch:1, current_iter:98, total iter:782] Avg Loss: 1.869 | Acc: 29.972 \n",
            "[epoch:1, current_iter:99, total iter:782] Avg Loss: 1.866 | Acc: 30.094 \n",
            "Epoch:1/20,step:100/782,Loss:1.488289\n",
            "[epoch:1, current_iter:100, total iter:782] Avg Loss: 1.867 | Acc: 30.012 \n",
            "[epoch:1, current_iter:101, total iter:782] Avg Loss: 1.866 | Acc: 29.994 \n",
            "[epoch:1, current_iter:102, total iter:782] Avg Loss: 1.866 | Acc: 29.945 \n",
            "[epoch:1, current_iter:103, total iter:782] Avg Loss: 1.863 | Acc: 30.063 \n",
            "[epoch:1, current_iter:104, total iter:782] Avg Loss: 1.864 | Acc: 30.030 \n",
            "[epoch:1, current_iter:105, total iter:782] Avg Loss: 1.862 | Acc: 30.056 \n",
            "[epoch:1, current_iter:106, total iter:782] Avg Loss: 1.861 | Acc: 30.082 \n",
            "[epoch:1, current_iter:107, total iter:782] Avg Loss: 1.859 | Acc: 30.136 \n",
            "[epoch:1, current_iter:108, total iter:782] Avg Loss: 1.860 | Acc: 30.146 \n",
            "[epoch:1, current_iter:109, total iter:782] Avg Loss: 1.860 | Acc: 30.071 \n",
            "[epoch:1, current_iter:110, total iter:782] Avg Loss: 1.859 | Acc: 30.124 \n",
            "[epoch:1, current_iter:111, total iter:782] Avg Loss: 1.858 | Acc: 30.162 \n",
            "[epoch:1, current_iter:112, total iter:782] Avg Loss: 1.857 | Acc: 30.171 \n",
            "[epoch:1, current_iter:113, total iter:782] Avg Loss: 1.855 | Acc: 30.236 \n",
            "[epoch:1, current_iter:114, total iter:782] Avg Loss: 1.854 | Acc: 30.285 \n",
            "[epoch:1, current_iter:115, total iter:782] Avg Loss: 1.854 | Acc: 30.321 \n",
            "[epoch:1, current_iter:116, total iter:782] Avg Loss: 1.853 | Acc: 30.395 \n",
            "[epoch:1, current_iter:117, total iter:782] Avg Loss: 1.852 | Acc: 30.363 \n",
            "[epoch:1, current_iter:118, total iter:782] Avg Loss: 1.850 | Acc: 30.488 \n",
            "[epoch:1, current_iter:119, total iter:782] Avg Loss: 1.849 | Acc: 30.521 \n",
            "[epoch:1, current_iter:120, total iter:782] Avg Loss: 1.847 | Acc: 30.630 \n",
            "[epoch:1, current_iter:121, total iter:782] Avg Loss: 1.845 | Acc: 30.789 \n",
            "[epoch:1, current_iter:122, total iter:782] Avg Loss: 1.844 | Acc: 30.907 \n",
            "[epoch:1, current_iter:123, total iter:782] Avg Loss: 1.845 | Acc: 30.885 \n",
            "[epoch:1, current_iter:124, total iter:782] Avg Loss: 1.843 | Acc: 30.913 \n",
            "[epoch:1, current_iter:125, total iter:782] Avg Loss: 1.841 | Acc: 31.002 \n",
            "[epoch:1, current_iter:126, total iter:782] Avg Loss: 1.838 | Acc: 31.164 \n",
            "[epoch:1, current_iter:127, total iter:782] Avg Loss: 1.837 | Acc: 31.226 \n",
            "[epoch:1, current_iter:128, total iter:782] Avg Loss: 1.835 | Acc: 31.286 \n",
            "[epoch:1, current_iter:129, total iter:782] Avg Loss: 1.833 | Acc: 31.394 \n",
            "[epoch:1, current_iter:130, total iter:782] Avg Loss: 1.832 | Acc: 31.441 \n",
            "[epoch:1, current_iter:131, total iter:782] Avg Loss: 1.830 | Acc: 31.617 \n",
            "[epoch:1, current_iter:132, total iter:782] Avg Loss: 1.829 | Acc: 31.638 \n",
            "[epoch:1, current_iter:133, total iter:782] Avg Loss: 1.829 | Acc: 31.611 \n",
            "[epoch:1, current_iter:134, total iter:782] Avg Loss: 1.827 | Acc: 31.644 \n",
            "[epoch:1, current_iter:135, total iter:782] Avg Loss: 1.827 | Acc: 31.641 \n",
            "[epoch:1, current_iter:136, total iter:782] Avg Loss: 1.827 | Acc: 31.615 \n",
            "[epoch:1, current_iter:137, total iter:782] Avg Loss: 1.825 | Acc: 31.726 \n",
            "[epoch:1, current_iter:138, total iter:782] Avg Loss: 1.824 | Acc: 31.711 \n",
            "[epoch:1, current_iter:139, total iter:782] Avg Loss: 1.823 | Acc: 31.763 \n",
            "[epoch:1, current_iter:140, total iter:782] Avg Loss: 1.823 | Acc: 31.738 \n",
            "[epoch:1, current_iter:141, total iter:782] Avg Loss: 1.822 | Acc: 31.767 \n",
            "[epoch:1, current_iter:142, total iter:782] Avg Loss: 1.820 | Acc: 31.851 \n",
            "[epoch:1, current_iter:143, total iter:782] Avg Loss: 1.820 | Acc: 31.890 \n",
            "[epoch:1, current_iter:144, total iter:782] Avg Loss: 1.818 | Acc: 31.950 \n",
            "[epoch:1, current_iter:145, total iter:782] Avg Loss: 1.818 | Acc: 31.924 \n",
            "[epoch:1, current_iter:146, total iter:782] Avg Loss: 1.815 | Acc: 31.994 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-103-e505328c9a68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-82-31aa16f8d710>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-79-a2999b2fa551>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_left\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mshortcut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshortcut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresidual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# print(r\"left shape:{},right shape:{}\".format(x.size(),shortcut.size()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    414\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    415\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 416\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_GOJkotMWuH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}